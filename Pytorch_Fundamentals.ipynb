{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7beb0ff-ac78-42ec-b7ed-d243e768ee6f",
   "metadata": {},
   "source": [
    "Hereâ€™s an updated version of the explanation and implementation using the latest PyTorch updates:\n",
    "\n",
    "---\n",
    "\n",
    "Before introducing PyTorch, we will first implement a neural network using **NumPy**. \n",
    "\n",
    "NumPy provides a powerful **n-dimensional array object** and many functions for manipulating these arrays. However, it doesn't handle **computation graphs**, **deep learning**, or **gradients**. Despite this, we can use NumPy to implement a basic two-layer network by manually coding the forward and backward passes:\n",
    "\n",
    "```python\n",
    "# Code: two_layer_net_numpy.py\n",
    "import numpy as np\n",
    "\n",
    "# Define network parameters\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10  # N: batch size, D_in: input dim, H: hidden dim, D_out: output dim\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "```\n",
    "\n",
    "### PyTorch: Tensors\n",
    "\n",
    "**NumPy** lacks the ability to utilize GPUs for computation acceleration, which is critical for deep learning. **PyTorch** fills this gap with **Tensors**, which are similar to NumPy arrays but can leverage GPUs.\n",
    "\n",
    "With the latest PyTorch updates, we can take advantage of improved features like automatic mixed precision and device management. Here's an updated version using PyTorch:\n",
    "\n",
    "```python\n",
    "# Code: two_layer_net_pytorch.py\n",
    "import torch\n",
    "\n",
    "# Set device (CUDA for GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Network parameters\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Initialize weights with requires_grad=True to enable autograd\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1)\n",
    "    h_relu = torch.relu(h)  # ReLU is now directly supported\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)  # Built-in MSE loss\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backward pass (automatic with PyTorch autograd)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using torch.no_grad() to avoid tracking this in autograd\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after each step\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "```\n",
    "\n",
    "### Latest Features Used:\n",
    "\n",
    "1. **Device management**: The code automatically switches between CPU and GPU (`torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`).\n",
    "2. **Built-in operations**: ReLU (`torch.relu`) and MSE Loss (`torch.nn.functional.mse_loss`) simplify the implementation.\n",
    "3. **Autograd**: PyTorch now fully automates the backward pass, so there's no need to manually compute gradients as with NumPy.\n",
    "4. **Memory Efficiency**: `torch.no_grad()` is used to prevent unnecessary graph tracking during the weight update step.\n",
    "\n",
    "### PyTorch: Automatic Mixed Precision (AMP)\n",
    "In the latest PyTorch versions, **Automatic Mixed Precision (AMP)** allows you to speed up training by using both FP16 and FP32 precision during computations. Here's how you can add AMP:\n",
    "\n",
    "```python\n",
    "# Code: two_layer_net_amp.py\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "scaler = GradScaler()  # For automatic mixed precision\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass under autocast for mixed precision\n",
    "    with autocast():\n",
    "        h = x.mm(w1)\n",
    "        h_relu = torch.relu(h)\n",
    "        y_pred = h_relu.mm(w2)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backward pass with scaler\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Update weights using scaler and no_grad\n",
    "    with torch.no_grad():\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "```\n",
    "\n",
    "This version demonstrates the **GradScaler** and **autocast** functionality, which optimizes performance on GPUs with mixed precision.\n",
    "\n",
    "With these updates, PyTorch now offers a more efficient and scalable way to build deep learning models, leveraging GPUs, mixed precision, and built-in loss functions and operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60c9d7-0e22-42f8-b548-c2c976dd95b0",
   "metadata": {},
   "source": [
    "In PyTorch, defining custom autograd functions allows users to create their own operations, including both forward and backward passes, providing more control over the computational graph. Here's an overview of how this can be achieved and contrasted with TensorFlow's static graph approach, followed by examples of using the `nn` package and optimizers in PyTorch.\n",
    "\n",
    "### PyTorch: Defining Custom Autograd Functions\n",
    "Under the hood, each primitive operator in autograd consists of two parts: the forward function, which computes the output tensors from input tensors, and the backward function, which computes gradients of input tensors based on the gradients of output tensors.\n",
    "\n",
    "To define a custom autograd operator in PyTorch, you can subclass `torch.autograd.Function` and implement both the forward and backward methods. Once the custom function is created, you can use it just like any other PyTorch operation.\n",
    "\n",
    "#### Example: Custom ReLU Autograd Function\n",
    "```python\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Cache the input tensor for the backward pass\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve the cached tensor from the context\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    "\n",
    "# Define input, output dimensions, and random data\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Use custom ReLU in forward pass\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "```\n",
    "\n",
    "### TensorFlow: Static Graphs vs PyTorch's Dynamic Graphs\n",
    "TensorFlow uses a static computation graph, where the graph is defined once and can be optimized before execution. In contrast, PyTorch uses dynamic computation graphs that are defined on-the-fly during each forward pass.\n",
    "\n",
    "Static graphs in TensorFlow can be optimized upfront, which can be efficient when running the same graph repeatedly. However, dynamic graphs allow for more flexibility, particularly for models with varying computation per input (e.g., recurrent networks with different sequence lengths).\n",
    "\n",
    "#### Example: TensorFlow Two-Layer Network\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Input, output dimensions and placeholders\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Weight variables\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2)\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Execute the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)\n",
    "```\n",
    "\n",
    "### PyTorch: Using `nn` for Layer Abstractions\n",
    "The `nn` package in PyTorch provides higher-level abstractions for neural networks, allowing users to define models using pre-built layers, loss functions, and optimizers.\n",
    "\n",
    "#### Example: Two-Layer Network using `nn`\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Input, output dimensions\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "```\n",
    "\n",
    "### PyTorch: Optimizers with `optim`\n",
    "Instead of manually updating weights, PyTorch's `optim` package provides abstractions for common optimization algorithms like SGD, Adam, RMSProp, etc.\n",
    "\n",
    "#### Example: Using Adam Optimizer\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Define model and loss function\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "This demonstrates how PyTorch's flexibility in dynamic graphs and its ease of use with `nn` and `optim` packages allow for powerful neural network modeling and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fe7f58-61c1-4b7f-8c85-44c7711bf96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37338025.02300994\n",
      "1 36577601.12595252\n",
      "2 36612852.627226725\n",
      "3 31416391.37271483\n",
      "4 21672867.698657844\n",
      "5 12078647.382830806\n",
      "6 6120143.2445837455\n",
      "7 3227548.8860854744\n",
      "8 1942409.0602344465\n",
      "9 1340734.2748368161\n",
      "10 1018983.8511665803\n",
      "11 818501.7659252007\n",
      "12 677312.4715301059\n",
      "13 569988.9290304305\n",
      "14 484695.2676211521\n",
      "15 415416.4135925022\n",
      "16 358200.31577261654\n",
      "17 310443.2739356288\n",
      "18 270330.1024791559\n",
      "19 236408.14543402605\n",
      "20 207499.9935934868\n",
      "21 182734.8407344056\n",
      "22 161416.89130377333\n",
      "23 142983.1814504591\n",
      "24 126993.11375855764\n",
      "25 113076.81753893393\n",
      "26 100918.8968384983\n",
      "27 90266.72630817394\n",
      "28 80906.02480269548\n",
      "29 72658.3331714608\n",
      "30 65371.61703061804\n",
      "31 58895.99662926106\n",
      "32 53150.023135610434\n",
      "33 48046.80486279775\n",
      "34 43496.858353863994\n",
      "35 39431.66462748908\n",
      "36 35794.92604247674\n",
      "37 32533.62496061922\n",
      "38 29604.40284832956\n",
      "39 26970.884536435264\n",
      "40 24597.360158995707\n",
      "41 22457.63785342501\n",
      "42 20525.32527871606\n",
      "43 18777.59615791388\n",
      "44 17194.76744142731\n",
      "45 15759.454538449638\n",
      "46 14457.819504134064\n",
      "47 13276.831465377782\n",
      "48 12202.185836792014\n",
      "49 11223.1011510817\n",
      "50 10332.965060952578\n",
      "51 9519.930013529016\n",
      "52 8777.09359930567\n",
      "53 8098.451103714733\n",
      "54 7477.752035214722\n",
      "55 6909.3683694153315\n",
      "56 6388.469255962885\n",
      "57 5911.322785060411\n",
      "58 5473.523988651553\n",
      "59 5070.996905259377\n",
      "60 4700.760460385273\n",
      "61 4360.223334757664\n",
      "62 4046.537245009057\n",
      "63 3757.37789896737\n",
      "64 3490.662105947603\n",
      "65 3244.4708389480093\n",
      "66 3017.147766643372\n",
      "67 2807.1204125260065\n",
      "68 2612.91068438419\n",
      "69 2433.2436081349288\n",
      "70 2266.8670587751385\n",
      "71 2112.8252855764304\n",
      "72 1970.0533472992279\n",
      "73 1837.6810514337776\n",
      "74 1714.9242046045524\n",
      "75 1600.9872343005925\n",
      "76 1495.1592902031882\n",
      "77 1396.8931672130254\n",
      "78 1305.6529610098266\n",
      "79 1220.7938765271865\n",
      "80 1142.3265075422737\n",
      "81 1069.38622965179\n",
      "82 1001.4155280657843\n",
      "83 938.0825000582703\n",
      "84 879.0381429109839\n",
      "85 823.9824988347086\n",
      "86 772.6070689725182\n",
      "87 724.6693192717747\n",
      "88 679.9180842793562\n",
      "89 638.0991563379972\n",
      "90 599.0240571377585\n",
      "91 562.5077463076047\n",
      "92 528.3528743321409\n",
      "93 496.4018051777856\n",
      "94 466.5024543301245\n",
      "95 438.51454956436396\n",
      "96 412.3053915026759\n",
      "97 387.76682777006215\n",
      "98 364.7742262488536\n",
      "99 343.22630045592956\n",
      "100 323.0239279568894\n",
      "101 304.0820251440379\n",
      "102 286.30901291594375\n",
      "103 269.6336250915167\n",
      "104 253.98067273381295\n",
      "105 239.28534279025519\n",
      "106 225.48910119239403\n",
      "107 212.5323856415602\n",
      "108 200.3544174916787\n",
      "109 188.9109380844618\n",
      "110 178.15211910806795\n",
      "111 168.04095460375385\n",
      "112 158.52891807805045\n",
      "113 149.57915270205615\n",
      "114 141.15919722177057\n",
      "115 133.2391403425103\n",
      "116 125.77811686397754\n",
      "117 118.76122004920664\n",
      "118 112.15764856774689\n",
      "119 105.93715390177188\n",
      "120 100.07499670135883\n",
      "121 94.55161267037141\n",
      "122 89.34570990395983\n",
      "123 84.43938411686187\n",
      "124 79.81191807390607\n",
      "125 75.44758776769275\n",
      "126 71.33091691897715\n",
      "127 67.44758517455162\n",
      "128 63.78325237637063\n",
      "129 60.32457939788906\n",
      "130 57.060093517166564\n",
      "131 53.981391926080185\n",
      "132 51.07279709647378\n",
      "133 48.32544498532414\n",
      "134 45.731548119112205\n",
      "135 43.281771335284446\n",
      "136 40.966325755350155\n",
      "137 38.77953682472469\n",
      "138 36.71341542538885\n",
      "139 34.76034477717544\n",
      "140 32.91383527270473\n",
      "141 31.16874623124768\n",
      "142 29.518935045807694\n",
      "143 27.958454998582187\n",
      "144 26.48277540007759\n",
      "145 25.08722286695523\n",
      "146 23.767297359539118\n",
      "147 22.518459753707287\n",
      "148 21.33677102599586\n",
      "149 20.21888777243838\n",
      "150 19.161112689711484\n",
      "151 18.160100691432493\n",
      "152 17.212484913413174\n",
      "153 16.315666480933025\n",
      "154 15.466405542206397\n",
      "155 14.662160931329622\n",
      "156 13.900792092534143\n",
      "157 13.179765243926976\n",
      "158 12.496914369054233\n",
      "159 11.850332949201647\n",
      "160 11.238069776090995\n",
      "161 10.657793885020501\n",
      "162 10.10819412959377\n",
      "163 9.587250754518166\n",
      "164 9.093725636156318\n",
      "165 8.626142228866783\n",
      "166 8.183097400810716\n",
      "167 7.763191288472355\n",
      "168 7.36521067060743\n",
      "169 6.988044333271849\n",
      "170 6.6303448488422685\n",
      "171 6.291287957404192\n",
      "172 5.969912134463083\n",
      "173 5.665254909413621\n",
      "174 5.376321423642991\n",
      "175 5.102371261218375\n",
      "176 4.842551165397445\n",
      "177 4.596188361198312\n",
      "178 4.362523678392591\n",
      "179 4.140952300214115\n",
      "180 3.930813862743709\n",
      "181 3.7313937135226163\n",
      "182 3.5422282670119785\n",
      "183 3.362786104026446\n",
      "184 3.1925304867055653\n",
      "185 3.031024921944137\n",
      "186 2.8779100848457047\n",
      "187 2.73252852662399\n",
      "188 2.594559983455161\n",
      "189 2.463656113729537\n",
      "190 2.3394051982012893\n",
      "191 2.221485865345185\n",
      "192 2.10960319337326\n",
      "193 2.003438892265798\n",
      "194 1.9026910123809384\n",
      "195 1.8070282021813011\n",
      "196 1.71620706920285\n",
      "197 1.6300044077032299\n",
      "198 1.5481793230603642\n",
      "199 1.470521046348173\n",
      "200 1.39678014457728\n",
      "201 1.3267544482552718\n",
      "202 1.260293467634692\n",
      "203 1.1972001809799535\n",
      "204 1.1372737775304476\n",
      "205 1.080385459854805\n",
      "206 1.0263672430800947\n",
      "207 0.9750615792274239\n",
      "208 0.9263376674986102\n",
      "209 0.8800787441245771\n",
      "210 0.836159954195125\n",
      "211 0.7944551398369432\n",
      "212 0.7548177290411875\n",
      "213 0.7171782712628147\n",
      "214 0.6814252874079708\n",
      "215 0.6474665858670492\n",
      "216 0.6152168628323732\n",
      "217 0.5845926635575343\n",
      "218 0.5555091804068046\n",
      "219 0.5278773464032116\n",
      "220 0.501620814133255\n",
      "221 0.4766783852921413\n",
      "222 0.4529839656506672\n",
      "223 0.430485811473458\n",
      "224 0.40909918210881\n",
      "225 0.3887843054040172\n",
      "226 0.3694920645729878\n",
      "227 0.3511545030380253\n",
      "228 0.33373045774371113\n",
      "229 0.3171804977412744\n",
      "230 0.30145217661156626\n",
      "231 0.28651245153957117\n",
      "232 0.2723133157896716\n",
      "233 0.2588253062034925\n",
      "234 0.246005516086162\n",
      "235 0.2338278047825879\n",
      "236 0.22225207001103653\n",
      "237 0.21125158029768318\n",
      "238 0.2007983229520208\n",
      "239 0.19086462105147628\n",
      "240 0.18142589673803655\n",
      "241 0.1724573718164603\n",
      "242 0.16393300996929286\n",
      "243 0.1558331123230339\n",
      "244 0.14813204672443886\n",
      "245 0.140813723500553\n",
      "246 0.1338584150277819\n",
      "247 0.12724874829721608\n",
      "248 0.12097045949856822\n",
      "249 0.114997913378065\n",
      "250 0.10932158268039999\n",
      "251 0.10392783602688033\n",
      "252 0.09879999557104494\n",
      "253 0.09392612124412089\n",
      "254 0.08929586270494128\n",
      "255 0.08489519352037136\n",
      "256 0.0807103575809337\n",
      "257 0.07673251754966769\n",
      "258 0.07295123941409822\n",
      "259 0.069356243700227\n",
      "260 0.06594056783051454\n",
      "261 0.06269179828477352\n",
      "262 0.059605214999199366\n",
      "263 0.056670743134392264\n",
      "264 0.05388008337732385\n",
      "265 0.051227354363050245\n",
      "266 0.04870736132758105\n",
      "267 0.046310037425714004\n",
      "268 0.044031294035886605\n",
      "269 0.04186639936948476\n",
      "270 0.039807275792067935\n",
      "271 0.03784906512963848\n",
      "272 0.03598800168705084\n",
      "273 0.03421824842074439\n",
      "274 0.03253575645934974\n",
      "275 0.030936438837453402\n",
      "276 0.029416287629662197\n",
      "277 0.02797117945132766\n",
      "278 0.026596864487866675\n",
      "279 0.02528995693524593\n",
      "280 0.02404723482398157\n",
      "281 0.022866026209593427\n",
      "282 0.021742852634207364\n",
      "283 0.020675393656837664\n",
      "284 0.019660192115870615\n",
      "285 0.01869484366209682\n",
      "286 0.0177768754122884\n",
      "287 0.016904085433862518\n",
      "288 0.016074540378589634\n",
      "289 0.015285502078084353\n",
      "290 0.014535714307664642\n",
      "291 0.013822379861023502\n",
      "292 0.013144033336509342\n",
      "293 0.012499221484317834\n",
      "294 0.011886090012303736\n",
      "295 0.011302994551965938\n",
      "296 0.010748590650724879\n",
      "297 0.010221671009032166\n",
      "298 0.009720390301276527\n",
      "299 0.00924380988676461\n",
      "300 0.008790444317724605\n",
      "301 0.0083594293146678\n",
      "302 0.007949557212354269\n",
      "303 0.007559984673712869\n",
      "304 0.0071895366465584976\n",
      "305 0.006837152453434929\n",
      "306 0.006501999699229612\n",
      "307 0.006183354754189495\n",
      "308 0.00588028360836782\n",
      "309 0.005592180396732165\n",
      "310 0.00531836931408808\n",
      "311 0.005057782469518197\n",
      "312 0.004809995870421685\n",
      "313 0.004574370367409363\n",
      "314 0.004350281070447414\n",
      "315 0.0041371892254450865\n",
      "316 0.003934580886979511\n",
      "317 0.0037420217281598127\n",
      "318 0.003558756278960013\n",
      "319 0.00338445733120464\n",
      "320 0.0032187558599290084\n",
      "321 0.0030611670676893273\n",
      "322 0.0029113085427072737\n",
      "323 0.0027687855764380025\n",
      "324 0.0026332777525626453\n",
      "325 0.002504348767935791\n",
      "326 0.0023817430414004195\n",
      "327 0.0022651533888739537\n",
      "328 0.0021542801826202472\n",
      "329 0.0020488658017184668\n",
      "330 0.001948651972033729\n",
      "331 0.0018533089814947875\n",
      "332 0.0017626125550153128\n",
      "333 0.00167634674772874\n",
      "334 0.0015943256797914438\n",
      "335 0.0015163233651065127\n",
      "336 0.0014421428273328355\n",
      "337 0.0013716078537121571\n",
      "338 0.0013044942652536472\n",
      "339 0.0012406686264382289\n",
      "340 0.0011799770308370304\n",
      "341 0.0011222914284444512\n",
      "342 0.0010673950332966624\n",
      "343 0.0010152102528949319\n",
      "344 0.0009655478430464133\n",
      "345 0.0009183198515092735\n",
      "346 0.0008734048472001752\n",
      "347 0.0008306905166964363\n",
      "348 0.0007900663648808443\n",
      "349 0.0007514409346073271\n",
      "350 0.0007147008454204902\n",
      "351 0.0006797635466978402\n",
      "352 0.0006465226955563405\n",
      "353 0.0006149119971070034\n",
      "354 0.0005848540603184369\n",
      "355 0.0005562594558619826\n",
      "356 0.0005290792440836933\n",
      "357 0.000503210160622499\n",
      "358 0.0004786063800540654\n",
      "359 0.00045521097669436617\n",
      "360 0.00043296061168146094\n",
      "361 0.0004118001628811971\n",
      "362 0.00039168178615478044\n",
      "363 0.00037253710114750307\n",
      "364 0.00035432614471427645\n",
      "365 0.00033700709152140786\n",
      "366 0.00032053813801122597\n",
      "367 0.0003048735064291771\n",
      "368 0.0002899754901089324\n",
      "369 0.0002758076759116048\n",
      "370 0.0002623285992009636\n",
      "371 0.0002495095696800606\n",
      "372 0.0002373184249029094\n",
      "373 0.0002257221219890015\n",
      "374 0.0002146904132931704\n",
      "375 0.0002042056514589523\n",
      "376 0.00019422669048286227\n",
      "377 0.00018473730528635867\n",
      "378 0.0001757116269692878\n",
      "379 0.00016712697050265906\n",
      "380 0.00015896217138831564\n",
      "381 0.00015119862883687803\n",
      "382 0.0001438140725301873\n",
      "383 0.00013678792314301524\n",
      "384 0.00013010386519606137\n",
      "385 0.00012374768691025866\n",
      "386 0.0001177028720496576\n",
      "387 0.00011195393630632344\n",
      "388 0.00010648691364339432\n",
      "389 0.00010128442001523871\n",
      "390 9.633625597777616e-05\n",
      "391 9.163166022628119e-05\n",
      "392 8.71569141259158e-05\n",
      "393 8.290008000054255e-05\n",
      "394 7.885267382352372e-05\n",
      "395 7.500076986336131e-05\n",
      "396 7.13373649906123e-05\n",
      "397 6.785289044121547e-05\n",
      "398 6.453843203712623e-05\n",
      "399 6.138717159445039e-05\n",
      "400 5.839076391284645e-05\n",
      "401 5.553958903550479e-05\n",
      "402 5.2827410129079794e-05\n",
      "403 5.024719412339493e-05\n",
      "404 4.779366788191699e-05\n",
      "405 4.545955892164726e-05\n",
      "406 4.3240624849940066e-05\n",
      "407 4.112935620338549e-05\n",
      "408 3.9120463632089485e-05\n",
      "409 3.721012925076323e-05\n",
      "410 3.5392804419553974e-05\n",
      "411 3.3664688854301164e-05\n",
      "412 3.2021914363771354e-05\n",
      "413 3.0458463078034593e-05\n",
      "414 2.8971139010727877e-05\n",
      "415 2.7556397977230596e-05\n",
      "416 2.6210762912683312e-05\n",
      "417 2.4931219957363035e-05\n",
      "418 2.3714061771246413e-05\n",
      "419 2.25567648583449e-05\n",
      "420 2.145527916783846e-05\n",
      "421 2.0407841217008184e-05\n",
      "422 1.941174341371119e-05\n",
      "423 1.8463823890490712e-05\n",
      "424 1.756244618724187e-05\n",
      "425 1.6705419491600592e-05\n",
      "426 1.588974932618529e-05\n",
      "427 1.5114078943459455e-05\n",
      "428 1.4376077922605034e-05\n",
      "429 1.3674161223085475e-05\n",
      "430 1.300669830487251e-05\n",
      "431 1.2372218403051164e-05\n",
      "432 1.1768350797350573e-05\n",
      "433 1.1193760819147134e-05\n",
      "434 1.0647269077906703e-05\n",
      "435 1.0127458612053081e-05\n",
      "436 9.632992206285447e-06\n",
      "437 9.163014725325617e-06\n",
      "438 8.715768975276285e-06\n",
      "439 8.290349050256374e-06\n",
      "440 7.885651268850462e-06\n",
      "441 7.500767843816612e-06\n",
      "442 7.134618264061182e-06\n",
      "443 6.786537669364036e-06\n",
      "444 6.455276454912073e-06\n",
      "445 6.140238783924201e-06\n",
      "446 5.840508037792887e-06\n",
      "447 5.555437153310298e-06\n",
      "448 5.284238859787019e-06\n",
      "449 5.026394601744636e-06\n",
      "450 4.781121207487795e-06\n",
      "451 4.547783671715421e-06\n",
      "452 4.32582894907042e-06\n",
      "453 4.114656321676765e-06\n",
      "454 3.913808538633496e-06\n",
      "455 3.7228460774567374e-06\n",
      "456 3.5411780448660095e-06\n",
      "457 3.36835669463182e-06\n",
      "458 3.203946324478697e-06\n",
      "459 3.047578223312104e-06\n",
      "460 2.8988772207947103e-06\n",
      "461 2.757431202616223e-06\n",
      "462 2.6228755874856967e-06\n",
      "463 2.4948507160460388e-06\n",
      "464 2.3730760460095502e-06\n",
      "465 2.2572588128320435e-06\n",
      "466 2.1470884470816763e-06\n",
      "467 2.0423370064782734e-06\n",
      "468 1.9426820731616805e-06\n",
      "469 1.847892441704311e-06\n",
      "470 1.7577496226669757e-06\n",
      "471 1.671948494782717e-06\n",
      "472 1.5903586802240094e-06\n",
      "473 1.5127667143828685e-06\n",
      "474 1.4389568161059254e-06\n",
      "475 1.3687354782833989e-06\n",
      "476 1.3019271877901219e-06\n",
      "477 1.2383859407705381e-06\n",
      "478 1.1779503072386848e-06\n",
      "479 1.1205083059012732e-06\n",
      "480 1.0658458329956443e-06\n",
      "481 1.0138230543618401e-06\n",
      "482 9.643466892919745e-07\n",
      "483 9.17288331177905e-07\n",
      "484 8.72521180555911e-07\n",
      "485 8.299593142768845e-07\n",
      "486 7.894672514973637e-07\n",
      "487 7.509395431563822e-07\n",
      "488 7.14295128827425e-07\n",
      "489 6.794488763744568e-07\n",
      "490 6.462901960683481e-07\n",
      "491 6.147603317656775e-07\n",
      "492 5.84769821215642e-07\n",
      "493 5.56234035410993e-07\n",
      "494 5.2909353099773e-07\n",
      "495 5.032729981688287e-07\n",
      "496 4.787124183641257e-07\n",
      "497 4.5536545832272667e-07\n",
      "498 4.3315296483044936e-07\n",
      "499 4.1202246785806514e-07\n"
     ]
    }
   ],
   "source": [
    "# Code: two_layer_net_numpy.py\n",
    "import numpy as np\n",
    "\n",
    "# Define network parameters\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10  # N: batch size, D_in: input dim, H: hidden dim, D_out: output dim\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff31c97-1870-44cf-b386-72e355fdaa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60853.15625\n",
      "1 60632.0390625\n",
      "2 60412.3125\n",
      "3 60194.0\n",
      "4 59977.07421875\n",
      "5 59761.51171875\n",
      "6 59547.32421875\n",
      "7 59334.5\n",
      "8 59123.02734375\n",
      "9 58912.8828125\n",
      "10 58704.0703125\n",
      "11 58496.57421875\n",
      "12 58290.38671875\n",
      "13 58085.5\n",
      "14 57881.9140625\n",
      "15 57679.6015625\n",
      "16 57478.5625\n",
      "17 57278.7890625\n",
      "18 57080.27734375\n",
      "19 56883.0\n",
      "20 56686.96484375\n",
      "21 56492.1640625\n",
      "22 56298.5703125\n",
      "23 56106.19921875\n",
      "24 55915.0390625\n",
      "25 55725.0625\n",
      "26 55536.27734375\n",
      "27 55348.6640625\n",
      "28 55162.21875\n",
      "29 54976.9375\n",
      "30 54792.8125\n",
      "31 54609.8203125\n",
      "32 54427.96875\n",
      "33 54247.24609375\n",
      "34 54067.63671875\n",
      "35 53889.15234375\n",
      "36 53711.76171875\n",
      "37 53535.4765625\n",
      "38 53360.26953125\n",
      "39 53186.14453125\n",
      "40 53013.09375\n",
      "41 52841.11328125\n",
      "42 52670.1875\n",
      "43 52500.3125\n",
      "44 52331.48046875\n",
      "45 52163.69140625\n",
      "46 51996.92578125\n",
      "47 51831.18359375\n",
      "48 51666.44921875\n",
      "49 51502.73046875\n",
      "50 51340.0078125\n",
      "51 51178.2734375\n",
      "52 51017.52734375\n",
      "53 50857.765625\n",
      "54 50698.97265625\n",
      "55 50541.15234375\n",
      "56 50384.28125\n",
      "57 50228.36328125\n",
      "58 50073.40234375\n",
      "59 49919.37109375\n",
      "60 49766.26953125\n",
      "61 49614.1015625\n",
      "62 49462.84375\n",
      "63 49312.50390625\n",
      "64 49163.0703125\n",
      "65 49014.5390625\n",
      "66 48866.8984375\n",
      "67 48720.14453125\n",
      "68 48574.27734375\n",
      "69 48429.28515625\n",
      "70 48285.17578125\n",
      "71 48141.96484375\n",
      "72 47999.60546875\n",
      "73 47858.10546875\n",
      "74 47717.44921875\n",
      "75 47577.6328125\n",
      "76 47438.65234375\n",
      "77 47300.48828125\n",
      "78 47163.1640625\n",
      "79 47026.640625\n",
      "80 46890.93359375\n",
      "81 46756.03125\n",
      "82 46621.93359375\n",
      "83 46488.62109375\n",
      "84 46356.10546875\n",
      "85 46224.3671875\n",
      "86 46093.40625\n",
      "87 45963.26171875\n",
      "88 45833.90234375\n",
      "89 45705.30078125\n",
      "90 45577.45703125\n",
      "91 45450.36328125\n",
      "92 45324.015625\n",
      "93 45198.41015625\n",
      "94 45073.54296875\n",
      "95 44949.40234375\n",
      "96 44825.98828125\n",
      "97 44703.29296875\n",
      "98 44581.3203125\n",
      "99 44460.05078125\n",
      "100 44339.484375\n",
      "101 44219.625\n",
      "102 44100.46484375\n",
      "103 43981.98828125\n",
      "104 43864.20703125\n",
      "105 43747.09765625\n",
      "106 43630.67578125\n",
      "107 43514.91796875\n",
      "108 43399.83203125\n",
      "109 43285.4140625\n",
      "110 43171.64453125\n",
      "111 43058.5390625\n",
      "112 42946.07421875\n",
      "113 42834.26171875\n",
      "114 42723.09375\n",
      "115 42612.55859375\n",
      "116 42502.65234375\n",
      "117 42393.37109375\n",
      "118 42284.71875\n",
      "119 42176.68359375\n",
      "120 42069.26953125\n",
      "121 41962.45703125\n",
      "122 41856.2578125\n",
      "123 41750.65625\n",
      "124 41645.6640625\n",
      "125 41541.25390625\n",
      "126 41437.4453125\n",
      "127 41334.21484375\n",
      "128 41231.5703125\n",
      "129 41129.5\n",
      "130 41028.0078125\n",
      "131 40927.09375\n",
      "132 40826.7421875\n",
      "133 40726.98828125\n",
      "134 40627.80859375\n",
      "135 40529.19921875\n",
      "136 40431.14453125\n",
      "137 40333.63671875\n",
      "138 40236.67578125\n",
      "139 40140.25390625\n",
      "140 40044.3671875\n",
      "141 39949.015625\n",
      "142 39854.1875\n",
      "143 39759.890625\n",
      "144 39666.11328125\n",
      "145 39572.85546875\n",
      "146 39480.1171875\n",
      "147 39387.88671875\n",
      "148 39296.16796875\n",
      "149 39204.94921875\n",
      "150 39114.23046875\n",
      "151 39024.015625\n",
      "152 38934.29296875\n",
      "153 38845.0625\n",
      "154 38756.3203125\n",
      "155 38668.0625\n",
      "156 38580.2890625\n",
      "157 38492.98828125\n",
      "158 38406.16796875\n",
      "159 38319.8125\n",
      "160 38233.93359375\n",
      "161 38148.51171875\n",
      "162 38063.55859375\n",
      "163 37979.0625\n",
      "164 37895.02734375\n",
      "165 37811.44921875\n",
      "166 37728.3125\n",
      "167 37645.62890625\n",
      "168 37563.3828125\n",
      "169 37481.58203125\n",
      "170 37400.21875\n",
      "171 37319.29296875\n",
      "172 37238.80078125\n",
      "173 37158.734375\n",
      "174 37079.1015625\n",
      "175 36999.8828125\n",
      "176 36921.08984375\n",
      "177 36842.71875\n",
      "178 36764.76171875\n",
      "179 36687.21484375\n",
      "180 36610.078125\n",
      "181 36533.3515625\n",
      "182 36457.02734375\n",
      "183 36381.10546875\n",
      "184 36305.58984375\n",
      "185 36230.4609375\n",
      "186 36155.73046875\n",
      "187 36081.38671875\n",
      "188 36007.4375\n",
      "189 35933.87109375\n",
      "190 35860.6953125\n",
      "191 35787.89453125\n",
      "192 35715.46875\n",
      "193 35643.421875\n",
      "194 35571.75\n",
      "195 35500.45703125\n",
      "196 35429.52734375\n",
      "197 35358.9609375\n",
      "198 35288.7578125\n",
      "199 35218.91796875\n",
      "200 35149.4453125\n",
      "201 35080.3203125\n",
      "202 35011.55078125\n",
      "203 34943.13671875\n",
      "204 34875.07421875\n",
      "205 34807.359375\n",
      "206 34739.98828125\n",
      "207 34672.95703125\n",
      "208 34606.26953125\n",
      "209 34539.921875\n",
      "210 34473.90625\n",
      "211 34408.2265625\n",
      "212 34342.875\n",
      "213 34277.85546875\n",
      "214 34213.16796875\n",
      "215 34148.79296875\n",
      "216 34084.7578125\n",
      "217 34021.03125\n",
      "218 33957.62890625\n",
      "219 33894.54296875\n",
      "220 33831.76953125\n",
      "221 33769.3125\n",
      "222 33707.1640625\n",
      "223 33645.32421875\n",
      "224 33583.7890625\n",
      "225 33522.5625\n",
      "226 33461.63671875\n",
      "227 33401.01171875\n",
      "228 33340.6875\n",
      "229 33280.66015625\n",
      "230 33220.9296875\n",
      "231 33161.484375\n",
      "232 33102.33984375\n",
      "233 33043.4765625\n",
      "234 32984.91015625\n",
      "235 32926.62109375\n",
      "236 32868.62109375\n",
      "237 32810.8984375\n",
      "238 32753.45703125\n",
      "239 32696.294921875\n",
      "240 32639.404296875\n",
      "241 32582.791015625\n",
      "242 32526.451171875\n",
      "243 32470.380859375\n",
      "244 32414.578125\n",
      "245 32359.046875\n",
      "246 32303.779296875\n",
      "247 32248.775390625\n",
      "248 32194.03125\n",
      "249 32139.552734375\n",
      "250 32085.328125\n",
      "251 32031.36328125\n",
      "252 31977.65625\n",
      "253 31924.201171875\n",
      "254 31871.0\n",
      "255 31818.05078125\n",
      "256 31765.34375\n",
      "257 31712.888671875\n",
      "258 31660.67578125\n",
      "259 31608.708984375\n",
      "260 31556.98828125\n",
      "261 31505.505859375\n",
      "262 31454.259765625\n",
      "263 31403.255859375\n",
      "264 31352.484375\n",
      "265 31301.953125\n",
      "266 31251.650390625\n",
      "267 31201.58203125\n",
      "268 31151.744140625\n",
      "269 31102.134765625\n",
      "270 31052.755859375\n",
      "271 31003.599609375\n",
      "272 30954.669921875\n",
      "273 30905.962890625\n",
      "274 30857.478515625\n",
      "275 30809.212890625\n",
      "276 30761.166015625\n",
      "277 30713.333984375\n",
      "278 30665.724609375\n",
      "279 30618.326171875\n",
      "280 30571.140625\n",
      "281 30524.166015625\n",
      "282 30477.40625\n",
      "283 30430.849609375\n",
      "284 30384.509765625\n",
      "285 30338.369140625\n",
      "286 30292.4375\n",
      "287 30246.71875\n",
      "288 30201.197265625\n",
      "289 30155.880859375\n",
      "290 30110.763671875\n",
      "291 30065.84375\n",
      "292 30021.123046875\n",
      "293 29976.599609375\n",
      "294 29932.26953125\n",
      "295 29888.130859375\n",
      "296 29844.1875\n",
      "297 29800.4375\n",
      "298 29756.87890625\n",
      "299 29713.505859375\n",
      "300 29670.322265625\n",
      "301 29627.326171875\n",
      "302 29584.513671875\n",
      "303 29541.880859375\n",
      "304 29499.44140625\n",
      "305 29457.177734375\n",
      "306 29415.09765625\n",
      "307 29373.197265625\n",
      "308 29331.474609375\n",
      "309 29289.931640625\n",
      "310 29248.560546875\n",
      "311 29207.365234375\n",
      "312 29166.349609375\n",
      "313 29125.50390625\n",
      "314 29084.83203125\n",
      "315 29044.33203125\n",
      "316 29004.00390625\n",
      "317 28963.841796875\n",
      "318 28923.849609375\n",
      "319 28884.01953125\n",
      "320 28844.359375\n",
      "321 28804.86328125\n",
      "322 28765.529296875\n",
      "323 28726.359375\n",
      "324 28687.349609375\n",
      "325 28648.50390625\n",
      "326 28609.81640625\n",
      "327 28571.28125\n",
      "328 28532.912109375\n",
      "329 28494.693359375\n",
      "330 28456.634765625\n",
      "331 28418.728515625\n",
      "332 28380.974609375\n",
      "333 28343.375\n",
      "334 28305.92578125\n",
      "335 28268.630859375\n",
      "336 28231.482421875\n",
      "337 28194.482421875\n",
      "338 28157.638671875\n",
      "339 28120.931640625\n",
      "340 28084.373046875\n",
      "341 28047.962890625\n",
      "342 28011.697265625\n",
      "343 27975.576171875\n",
      "344 27939.59375\n",
      "345 27903.75390625\n",
      "346 27868.056640625\n",
      "347 27832.498046875\n",
      "348 27797.076171875\n",
      "349 27761.794921875\n",
      "350 27726.650390625\n",
      "351 27691.64453125\n",
      "352 27656.771484375\n",
      "353 27622.03125\n",
      "354 27587.427734375\n",
      "355 27552.962890625\n",
      "356 27518.625\n",
      "357 27484.419921875\n",
      "358 27450.34375\n",
      "359 27416.400390625\n",
      "360 27382.583984375\n",
      "361 27348.896484375\n",
      "362 27315.341796875\n",
      "363 27281.90625\n",
      "364 27248.607421875\n",
      "365 27215.42578125\n",
      "366 27182.369140625\n",
      "367 27149.443359375\n",
      "368 27116.65625\n",
      "369 27083.994140625\n",
      "370 27051.447265625\n",
      "371 27019.029296875\n",
      "372 26986.724609375\n",
      "373 26954.544921875\n",
      "374 26922.482421875\n",
      "375 26890.53125\n",
      "376 26858.703125\n",
      "377 26826.994140625\n",
      "378 26795.39453125\n",
      "379 26763.912109375\n",
      "380 26732.544921875\n",
      "381 26701.294921875\n",
      "382 26670.16015625\n",
      "383 26639.134765625\n",
      "384 26608.224609375\n",
      "385 26577.42578125\n",
      "386 26546.732421875\n",
      "387 26516.150390625\n",
      "388 26485.677734375\n",
      "389 26455.3125\n",
      "390 26425.0625\n",
      "391 26394.91015625\n",
      "392 26364.869140625\n",
      "393 26334.931640625\n",
      "394 26305.099609375\n",
      "395 26275.369140625\n",
      "396 26245.75\n",
      "397 26216.232421875\n",
      "398 26186.810546875\n",
      "399 26157.498046875\n",
      "400 26128.28515625\n",
      "401 26099.17578125\n",
      "402 26070.162109375\n",
      "403 26041.248046875\n",
      "404 26012.43359375\n",
      "405 25983.71875\n",
      "406 25955.099609375\n",
      "407 25926.583984375\n",
      "408 25898.15625\n",
      "409 25869.83203125\n",
      "410 25841.599609375\n",
      "411 25813.462890625\n",
      "412 25785.419921875\n",
      "413 25757.47265625\n",
      "414 25729.6171875\n",
      "415 25701.85546875\n",
      "416 25674.185546875\n",
      "417 25646.607421875\n",
      "418 25619.119140625\n",
      "419 25591.724609375\n",
      "420 25564.419921875\n",
      "421 25537.203125\n",
      "422 25510.076171875\n",
      "423 25483.037109375\n",
      "424 25456.087890625\n",
      "425 25429.22265625\n",
      "426 25402.447265625\n",
      "427 25375.755859375\n",
      "428 25349.154296875\n",
      "429 25322.63671875\n",
      "430 25296.203125\n",
      "431 25269.85546875\n",
      "432 25243.587890625\n",
      "433 25217.408203125\n",
      "434 25191.310546875\n",
      "435 25165.294921875\n",
      "436 25139.36328125\n",
      "437 25113.509765625\n",
      "438 25087.740234375\n",
      "439 25062.05078125\n",
      "440 25036.443359375\n",
      "441 25010.912109375\n",
      "442 24985.4609375\n",
      "443 24960.087890625\n",
      "444 24934.794921875\n",
      "445 24909.58203125\n",
      "446 24884.443359375\n",
      "447 24859.380859375\n",
      "448 24834.396484375\n",
      "449 24809.48828125\n",
      "450 24784.65625\n",
      "451 24759.900390625\n",
      "452 24735.216796875\n",
      "453 24710.609375\n",
      "454 24686.076171875\n",
      "455 24661.61328125\n",
      "456 24637.228515625\n",
      "457 24612.916015625\n",
      "458 24588.67578125\n",
      "459 24564.505859375\n",
      "460 24540.4140625\n",
      "461 24516.39453125\n",
      "462 24492.443359375\n",
      "463 24468.560546875\n",
      "464 24444.75\n",
      "465 24421.005859375\n",
      "466 24397.337890625\n",
      "467 24373.732421875\n",
      "468 24350.19921875\n",
      "469 24326.732421875\n",
      "470 24303.33203125\n",
      "471 24280.0\n",
      "472 24256.734375\n",
      "473 24233.53515625\n",
      "474 24210.404296875\n",
      "475 24187.333984375\n",
      "476 24164.33203125\n",
      "477 24141.396484375\n",
      "478 24118.525390625\n",
      "479 24095.716796875\n",
      "480 24072.97265625\n",
      "481 24050.291015625\n",
      "482 24027.67578125\n",
      "483 24005.123046875\n",
      "484 23982.62890625\n",
      "485 23960.19921875\n",
      "486 23937.830078125\n",
      "487 23915.521484375\n",
      "488 23893.279296875\n",
      "489 23871.09375\n",
      "490 23848.96875\n",
      "491 23826.904296875\n",
      "492 23804.900390625\n",
      "493 23782.95703125\n",
      "494 23761.072265625\n",
      "495 23739.24609375\n",
      "496 23717.474609375\n",
      "497 23695.765625\n",
      "498 23674.115234375\n",
      "499 23652.51953125\n"
     ]
    }
   ],
   "source": [
    "# Code: two_layer_net_pytorch.py\n",
    "import torch\n",
    "\n",
    "# Set device (CUDA for GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Network parameters\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Initialize weights with requires_grad=True to enable autograd\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1)\n",
    "    h_relu = torch.relu(h)  # ReLU is now directly supported\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)  # Built-in MSE loss\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backward pass (automatic with PyTorch autograd)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using torch.no_grad() to avoid tracking this in autograd\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after each step\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c19176-cc51-4d64-95ec-a2b6b73c30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 651.315185546875\n",
      "1 635.20068359375\n",
      "2 619.5158081054688\n",
      "3 604.2950439453125\n",
      "4 589.5348510742188\n",
      "5 575.2011108398438\n",
      "6 561.2222900390625\n",
      "7 547.69189453125\n",
      "8 534.544921875\n",
      "9 521.8331909179688\n",
      "10 509.4169616699219\n",
      "11 497.3133239746094\n",
      "12 485.4803771972656\n",
      "13 473.89947509765625\n",
      "14 462.5671691894531\n",
      "15 451.5332336425781\n",
      "16 440.76434326171875\n",
      "17 430.3234558105469\n",
      "18 420.21759033203125\n",
      "19 410.3592529296875\n",
      "20 400.739501953125\n",
      "21 391.3398742675781\n",
      "22 382.22027587890625\n",
      "23 373.3459777832031\n",
      "24 364.6662902832031\n",
      "25 356.2093505859375\n",
      "26 347.9697265625\n",
      "27 339.9106140136719\n",
      "28 332.00531005859375\n",
      "29 324.2637939453125\n",
      "30 316.7051086425781\n",
      "31 309.3155517578125\n",
      "32 302.0869445800781\n",
      "33 295.03125\n",
      "34 288.1441345214844\n",
      "35 281.4339294433594\n",
      "36 274.888916015625\n",
      "37 268.470458984375\n",
      "38 262.17535400390625\n",
      "39 256.0076904296875\n",
      "40 249.972900390625\n",
      "41 244.0723419189453\n",
      "42 238.29501342773438\n",
      "43 232.60977172851562\n",
      "44 227.01943969726562\n",
      "45 221.54942321777344\n",
      "46 216.1769256591797\n",
      "47 210.9066925048828\n",
      "48 205.76165771484375\n",
      "49 200.7421112060547\n",
      "50 195.81553649902344\n",
      "51 191.00038146972656\n",
      "52 186.27255249023438\n",
      "53 181.6495361328125\n",
      "54 177.11329650878906\n",
      "55 172.6559600830078\n",
      "56 168.28488159179688\n",
      "57 163.99130249023438\n",
      "58 159.7840118408203\n",
      "59 155.6650390625\n",
      "60 151.62794494628906\n",
      "61 147.6593017578125\n",
      "62 143.7690887451172\n",
      "63 139.9644012451172\n",
      "64 136.24130249023438\n",
      "65 132.59605407714844\n",
      "66 129.0218048095703\n",
      "67 125.53855895996094\n",
      "68 122.12739562988281\n",
      "69 118.7997817993164\n",
      "70 115.54386901855469\n",
      "71 112.34483337402344\n",
      "72 109.2225570678711\n",
      "73 106.17285919189453\n",
      "74 103.20211791992188\n",
      "75 100.28715515136719\n",
      "76 97.44100189208984\n",
      "77 94.65582275390625\n",
      "78 91.93407440185547\n",
      "79 89.27668762207031\n",
      "80 86.677978515625\n",
      "81 84.13640594482422\n",
      "82 81.64681243896484\n",
      "83 79.2187271118164\n",
      "84 76.85090637207031\n",
      "85 74.5340576171875\n",
      "86 72.26172637939453\n",
      "87 70.047119140625\n",
      "88 67.88322448730469\n",
      "89 65.77001190185547\n",
      "90 63.70402526855469\n",
      "91 61.684898376464844\n",
      "92 59.71759033203125\n",
      "93 57.80073547363281\n",
      "94 55.92958068847656\n",
      "95 54.107749938964844\n",
      "96 52.332984924316406\n",
      "97 50.604286193847656\n",
      "98 48.92083740234375\n",
      "99 47.27915954589844\n",
      "100 45.6831169128418\n",
      "101 44.1265869140625\n",
      "102 42.61334991455078\n",
      "103 41.142696380615234\n",
      "104 39.71461486816406\n",
      "105 38.325927734375\n",
      "106 36.97374725341797\n",
      "107 35.66118621826172\n",
      "108 34.38768768310547\n",
      "109 33.150634765625\n",
      "110 31.95302963256836\n",
      "111 30.785411834716797\n",
      "112 29.65188980102539\n",
      "113 28.553085327148438\n",
      "114 27.491783142089844\n",
      "115 26.46242904663086\n",
      "116 25.464462280273438\n",
      "117 24.49800682067871\n",
      "118 23.55984115600586\n",
      "119 22.65020179748535\n",
      "120 21.769855499267578\n",
      "121 20.916200637817383\n",
      "122 20.08963966369629\n",
      "123 19.289859771728516\n",
      "124 18.516708374023438\n",
      "125 17.769367218017578\n",
      "126 17.04574203491211\n",
      "127 16.34649085998535\n",
      "128 15.67137622833252\n",
      "129 15.020498275756836\n",
      "130 14.391887664794922\n",
      "131 13.784631729125977\n",
      "132 13.199501991271973\n",
      "133 12.635980606079102\n",
      "134 12.091718673706055\n",
      "135 11.567824363708496\n",
      "136 11.063628196716309\n",
      "137 10.578275680541992\n",
      "138 10.110702514648438\n",
      "139 9.66094970703125\n",
      "140 9.229104042053223\n",
      "141 8.813579559326172\n",
      "142 8.414839744567871\n",
      "143 8.03155517578125\n",
      "144 7.663871765136719\n",
      "145 7.3106794357299805\n",
      "146 6.972029209136963\n",
      "147 6.647337436676025\n",
      "148 6.336369514465332\n",
      "149 6.038412094116211\n",
      "150 5.75333309173584\n",
      "151 5.480210304260254\n",
      "152 5.218711853027344\n",
      "153 4.9688496589660645\n",
      "154 4.730095386505127\n",
      "155 4.501535892486572\n",
      "156 4.283229827880859\n",
      "157 4.074296951293945\n",
      "158 3.8750057220458984\n",
      "159 3.68452787399292\n",
      "160 3.5028076171875\n",
      "161 3.329512596130371\n",
      "162 3.1639821529388428\n",
      "163 3.0059683322906494\n",
      "164 2.8552937507629395\n",
      "165 2.7118728160858154\n",
      "166 2.5751805305480957\n",
      "167 2.4447779655456543\n",
      "168 2.3205792903900146\n",
      "169 2.2022879123687744\n",
      "170 2.0897796154022217\n",
      "171 1.982776403427124\n",
      "172 1.8809332847595215\n",
      "173 1.7839322090148926\n",
      "174 1.6917283535003662\n",
      "175 1.6039496660232544\n",
      "176 1.5204867124557495\n",
      "177 1.4412389993667603\n",
      "178 1.3659076690673828\n",
      "179 1.294264316558838\n",
      "180 1.2261667251586914\n",
      "181 1.16150963306427\n",
      "182 1.100132942199707\n",
      "183 1.0418906211853027\n",
      "184 0.9865280985832214\n",
      "185 0.9339282512664795\n",
      "186 0.8840013742446899\n",
      "187 0.8366121649742126\n",
      "188 0.7916969060897827\n",
      "189 0.7490543723106384\n",
      "190 0.7085742950439453\n",
      "191 0.6702118515968323\n",
      "192 0.6338756680488586\n",
      "193 0.5994569063186646\n",
      "194 0.5667990446090698\n",
      "195 0.5358687043190002\n",
      "196 0.5066245794296265\n",
      "197 0.47916659712791443\n",
      "198 0.45313090085983276\n",
      "199 0.42851555347442627\n",
      "200 0.4052031636238098\n",
      "201 0.38312065601348877\n",
      "202 0.36223068833351135\n",
      "203 0.3424612879753113\n",
      "204 0.3237364888191223\n",
      "205 0.30602484941482544\n",
      "206 0.2892715036869049\n",
      "207 0.27345460653305054\n",
      "208 0.2584795653820038\n",
      "209 0.24430784583091736\n",
      "210 0.23092983663082123\n",
      "211 0.2182706594467163\n",
      "212 0.20629778504371643\n",
      "213 0.19500617682933807\n",
      "214 0.18429993093013763\n",
      "215 0.17419637739658356\n",
      "216 0.16464978456497192\n",
      "217 0.1556256264448166\n",
      "218 0.14710697531700134\n",
      "219 0.1390552669763565\n",
      "220 0.1314481496810913\n",
      "221 0.1242622584104538\n",
      "222 0.11747189611196518\n",
      "223 0.11105793714523315\n",
      "224 0.10499845445156097\n",
      "225 0.09927050024271011\n",
      "226 0.0938582643866539\n",
      "227 0.08874579519033432\n",
      "228 0.08391663432121277\n",
      "229 0.07935498654842377\n",
      "230 0.07504511624574661\n",
      "231 0.07097013294696808\n",
      "232 0.06712231785058975\n",
      "233 0.06348589062690735\n",
      "234 0.06004893034696579\n",
      "235 0.05680014193058014\n",
      "236 0.0537322461605072\n",
      "237 0.05083256587386131\n",
      "238 0.048091452568769455\n",
      "239 0.04550107941031456\n",
      "240 0.04305034875869751\n",
      "241 0.04073375090956688\n",
      "242 0.038544390350580215\n",
      "243 0.03647376596927643\n",
      "244 0.03451747074723244\n",
      "245 0.03266558051109314\n",
      "246 0.03091479279100895\n",
      "247 0.02925768867135048\n",
      "248 0.027690492570400238\n",
      "249 0.026208482682704926\n",
      "250 0.024806039407849312\n",
      "251 0.023481514304876328\n",
      "252 0.022225163877010345\n",
      "253 0.021038446575403214\n",
      "254 0.019915830343961716\n",
      "255 0.018853770568966866\n",
      "256 0.017848916351795197\n",
      "257 0.016898686066269875\n",
      "258 0.015999287366867065\n",
      "259 0.015148820355534554\n",
      "260 0.014342933893203735\n",
      "261 0.01358101237565279\n",
      "262 0.012859849259257317\n",
      "263 0.012177305296063423\n",
      "264 0.011531498283147812\n",
      "265 0.010920309461653233\n",
      "266 0.010344886220991611\n",
      "267 0.009811108000576496\n",
      "268 0.009305888786911964\n",
      "269 0.008828045800328255\n",
      "270 0.008375818841159344\n",
      "271 0.007947759702801704\n",
      "272 0.007542657665908337\n",
      "273 0.007159358821809292\n",
      "274 0.0067969439551234245\n",
      "275 0.0064527662470936775\n",
      "276 0.0061273351311683655\n",
      "277 0.005819171667098999\n",
      "278 0.005527077708393335\n",
      "279 0.005250493995845318\n",
      "280 0.004989217966794968\n",
      "281 0.004742044024169445\n",
      "282 0.004507711157202721\n",
      "283 0.004285650793462992\n",
      "284 0.0040750885382294655\n",
      "285 0.003875389462336898\n",
      "286 0.0036859624087810516\n",
      "287 0.0035061705857515335\n",
      "288 0.0033356421627104282\n",
      "289 0.003173816716298461\n",
      "290 0.0030202039051800966\n",
      "291 0.002874325029551983\n",
      "292 0.0027358876541256905\n",
      "293 0.0026042493991553783\n",
      "294 0.002479315735399723\n",
      "295 0.002360587939620018\n",
      "296 0.002247803146019578\n",
      "297 0.0021405848674476147\n",
      "298 0.00203861971385777\n",
      "299 0.0019417149014770985\n",
      "300 0.0018495485419407487\n",
      "301 0.001761921215802431\n",
      "302 0.0016785034677013755\n",
      "303 0.0015992112457752228\n",
      "304 0.0015237202169373631\n",
      "305 0.0014518906828016043\n",
      "306 0.0013835523277521133\n",
      "307 0.001318449736572802\n",
      "308 0.0012564928038045764\n",
      "309 0.001197516336105764\n",
      "310 0.0011413497850298882\n",
      "311 0.0010878583416342735\n",
      "312 0.0010368955554440618\n",
      "313 0.0009883507154881954\n",
      "314 0.0009421188151463866\n",
      "315 0.0008980606216937304\n",
      "316 0.0008560880669392645\n",
      "317 0.0008160736178979278\n",
      "318 0.0007779548177495599\n",
      "319 0.0007416262524202466\n",
      "320 0.0007069968269206583\n",
      "321 0.0006739789969287813\n",
      "322 0.0006425126339308918\n",
      "323 0.0006125372601673007\n",
      "324 0.000583927147090435\n",
      "325 0.000556674029212445\n",
      "326 0.0005306735984049737\n",
      "327 0.0005058986716903746\n",
      "328 0.0004822671180590987\n",
      "329 0.0004597249790094793\n",
      "330 0.00043823497253470123\n",
      "331 0.00041775027057155967\n",
      "332 0.0003981944464612752\n",
      "333 0.00037956651067361236\n",
      "334 0.00036179253947921097\n",
      "335 0.0003448382194619626\n",
      "336 0.0003286561695858836\n",
      "337 0.00031323847360908985\n",
      "338 0.0002985307655762881\n",
      "339 0.00028450292302295566\n",
      "340 0.00027111751842312515\n",
      "341 0.00025835406268015504\n",
      "342 0.00024619128089398146\n",
      "343 0.00023457709176000208\n",
      "344 0.00022349448408931494\n",
      "345 0.0002129403583239764\n",
      "346 0.0002028581511694938\n",
      "347 0.00019325126777403057\n",
      "348 0.00018407945754006505\n",
      "349 0.00017534379730932415\n",
      "350 0.00016700357082299888\n",
      "351 0.00015905134205240756\n",
      "352 0.00015147114754654467\n",
      "353 0.0001442403590772301\n",
      "354 0.00013734353706240654\n",
      "355 0.0001307696511503309\n",
      "356 0.00012450263602659106\n",
      "357 0.00011852567695314065\n",
      "358 0.00011282434570603073\n",
      "359 0.00010740092693595216\n",
      "360 0.00010222001583315432\n",
      "361 9.728249278850853e-05\n",
      "362 9.257437341148034e-05\n",
      "363 8.808667189441621e-05\n",
      "364 8.381469524465501e-05\n",
      "365 7.973664469318464e-05\n",
      "366 7.585517596453428e-05\n",
      "367 7.21564210834913e-05\n",
      "368 6.863127055112273e-05\n",
      "369 6.526699144160375e-05\n",
      "370 6.206923717400059e-05\n",
      "371 5.90177187405061e-05\n",
      "372 5.611608867184259e-05\n",
      "373 5.334678280632943e-05\n",
      "374 5.071568739367649e-05\n",
      "375 4.820661342819221e-05\n",
      "376 4.581488610710949e-05\n",
      "377 4.354128395789303e-05\n",
      "378 4.1374289139639586e-05\n",
      "379 3.931000901502557e-05\n",
      "380 3.73469629266765e-05\n",
      "381 3.548051245161332e-05\n",
      "382 3.3700078347465023e-05\n",
      "383 3.200671199010685e-05\n",
      "384 3.0396869988180697e-05\n",
      "385 2.886588299588766e-05\n",
      "386 2.7405694709159434e-05\n",
      "387 2.6014906325144693e-05\n",
      "388 2.4697143089724705e-05\n",
      "389 2.344090535189025e-05\n",
      "390 2.224796116934158e-05\n",
      "391 2.1111776732141152e-05\n",
      "392 2.0032848624396138e-05\n",
      "393 1.900394090625923e-05\n",
      "394 1.8028753402177244e-05\n",
      "395 1.710241849650629e-05\n",
      "396 1.6221012629102916e-05\n",
      "397 1.5382185665657744e-05\n",
      "398 1.4586652469006367e-05\n",
      "399 1.3828936062054709e-05\n",
      "400 1.3110340660205111e-05\n",
      "401 1.242791950062383e-05\n",
      "402 1.1778762200265191e-05\n",
      "403 1.1161178917973302e-05\n",
      "404 1.0577160537650343e-05\n",
      "405 1.0022544302046299e-05\n",
      "406 9.494099685980473e-06\n",
      "407 8.99380938790273e-06\n",
      "408 8.519031325704418e-06\n",
      "409 8.066179361776449e-06\n",
      "410 7.638205715920776e-06\n",
      "411 7.233491487568244e-06\n",
      "412 6.84582755638985e-06\n",
      "413 6.4808514252945315e-06\n",
      "414 6.134205250418745e-06\n",
      "415 5.804889497085242e-06\n",
      "416 5.493169282999588e-06\n",
      "417 5.197144673729781e-06\n",
      "418 4.916036687063752e-06\n",
      "419 4.650677965400973e-06\n",
      "420 4.398164946906036e-06\n",
      "421 4.159291620453587e-06\n",
      "422 3.932939307560446e-06\n",
      "423 3.7183785934757907e-06\n",
      "424 3.5149626000929857e-06\n",
      "425 3.322021029816824e-06\n",
      "426 3.139859700240777e-06\n",
      "427 2.9679740691790357e-06\n",
      "428 2.804115410981467e-06\n",
      "429 2.649112957442412e-06\n",
      "430 2.5015024220920168e-06\n",
      "431 2.363472731303773e-06\n",
      "432 2.231773351013544e-06\n",
      "433 2.107524096572888e-06\n",
      "434 1.989628572118818e-06\n",
      "435 1.8786522559821606e-06\n",
      "436 1.7730200170262833e-06\n",
      "437 1.673642600508174e-06\n",
      "438 1.579253762429289e-06\n",
      "439 1.490283807470405e-06\n",
      "440 1.4059229442864307e-06\n",
      "441 1.3262093716548407e-06\n",
      "442 1.2510820397437783e-06\n",
      "443 1.1796817034337437e-06\n",
      "444 1.1122440355393337e-06\n",
      "445 1.0486864994163625e-06\n",
      "446 9.885544614007813e-07\n",
      "447 9.316214573118486e-07\n",
      "448 8.779354061516642e-07\n",
      "449 8.277971801362582e-07\n",
      "450 7.795961209922098e-07\n",
      "451 7.347240398303256e-07\n",
      "452 6.922821853549976e-07\n",
      "453 6.517525434901472e-07\n",
      "454 6.138778303466097e-07\n",
      "455 5.780959213552705e-07\n",
      "456 5.442820452117303e-07\n",
      "457 5.120152763993246e-07\n",
      "458 4.819311811843363e-07\n",
      "459 4.538485995908559e-07\n",
      "460 4.2669941535677935e-07\n",
      "461 4.0141190993381315e-07\n",
      "462 3.780099575578788e-07\n",
      "463 3.553276428647223e-07\n",
      "464 3.341011165503005e-07\n",
      "465 3.1425813062924135e-07\n",
      "466 2.9587528160845977e-07\n",
      "467 2.779871408620238e-07\n",
      "468 2.6119570861737884e-07\n",
      "469 2.4559221856179647e-07\n",
      "470 2.3075838839758944e-07\n",
      "471 2.167030572763906e-07\n",
      "472 2.0364319652799168e-07\n",
      "473 1.9117331362394907e-07\n",
      "474 1.7964062237751932e-07\n",
      "475 1.6867710428414284e-07\n",
      "476 1.5847459167162015e-07\n",
      "477 1.4885046084600617e-07\n",
      "478 1.396740998416135e-07\n",
      "479 1.3108996199662215e-07\n",
      "480 1.2295379292481812e-07\n",
      "481 1.1535487232094965e-07\n",
      "482 1.0827658059042733e-07\n",
      "483 1.016363739836379e-07\n",
      "484 9.533384570659109e-08\n",
      "485 8.945253426873023e-08\n",
      "486 8.393922001914689e-08\n",
      "487 7.868951712453054e-08\n",
      "488 7.369344956487112e-08\n",
      "489 6.895029258657814e-08\n",
      "490 6.46868585363336e-08\n",
      "491 6.065076973982286e-08\n",
      "492 5.683502735109869e-08\n",
      "493 5.321660978552245e-08\n",
      "494 4.996797997591784e-08\n",
      "495 4.676527254332541e-08\n",
      "496 4.3777799163535747e-08\n",
      "497 4.1095891134546036e-08\n",
      "498 3.843064888542358e-08\n",
      "499 3.598644582325505e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights\n",
    "# of the model for us. Here we will use Adam optimizer.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all learnable parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters using the optimizer\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e67776-99da-4d1f-9648-1e66326c18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50592.66796875\n",
      "1 50592.66796875\n",
      "2 50592.66796875\n",
      "3 50592.66796875\n",
      "4 50592.66796875\n",
      "5 50592.66796875\n",
      "6 50592.66796875\n",
      "7 50592.66796875\n",
      "8 50592.66796875\n",
      "9 50592.66796875\n",
      "10 50592.66796875\n",
      "11 50592.66796875\n",
      "12 50592.66796875\n",
      "13 50592.66796875\n",
      "14 50592.66796875\n",
      "15 50592.66796875\n",
      "16 50592.66796875\n",
      "17 50592.66796875\n",
      "18 50592.66796875\n",
      "19 50592.66796875\n",
      "20 50592.66796875\n",
      "21 50592.66796875\n",
      "22 50592.66796875\n",
      "23 50592.66796875\n",
      "24 50592.66796875\n",
      "25 50592.66796875\n",
      "26 50592.66796875\n",
      "27 50592.66796875\n",
      "28 50592.66796875\n",
      "29 50592.66796875\n",
      "30 50592.66796875\n",
      "31 50592.66796875\n",
      "32 50592.66796875\n",
      "33 50592.66796875\n",
      "34 50592.66796875\n",
      "35 50592.66796875\n",
      "36 50592.66796875\n",
      "37 50592.66796875\n",
      "38 50592.66796875\n",
      "39 50592.66796875\n",
      "40 50592.66796875\n",
      "41 50592.66796875\n",
      "42 50592.66796875\n",
      "43 50592.66796875\n",
      "44 50592.66796875\n",
      "45 50592.66796875\n",
      "46 50592.66796875\n",
      "47 50592.66796875\n",
      "48 50592.66796875\n",
      "49 50592.66796875\n",
      "50 50592.66796875\n",
      "51 50592.66796875\n",
      "52 50592.66796875\n",
      "53 50592.66796875\n",
      "54 50592.66796875\n",
      "55 50592.66796875\n",
      "56 50592.66796875\n",
      "57 50592.66796875\n",
      "58 50592.66796875\n",
      "59 50592.66796875\n",
      "60 50592.66796875\n",
      "61 50592.66796875\n",
      "62 50592.66796875\n",
      "63 50592.66796875\n",
      "64 50592.66796875\n",
      "65 50592.66796875\n",
      "66 50592.66796875\n",
      "67 50592.66796875\n",
      "68 50592.66796875\n",
      "69 50592.66796875\n",
      "70 50592.66796875\n",
      "71 50592.66796875\n",
      "72 50592.66796875\n",
      "73 50592.66796875\n",
      "74 50592.66796875\n",
      "75 50592.66796875\n",
      "76 50592.66796875\n",
      "77 50592.66796875\n",
      "78 50592.66796875\n",
      "79 50592.66796875\n",
      "80 50592.66796875\n",
      "81 50592.66796875\n",
      "82 50592.66796875\n",
      "83 50592.66796875\n",
      "84 50592.66796875\n",
      "85 50592.66796875\n",
      "86 50592.66796875\n",
      "87 50592.66796875\n",
      "88 50592.66796875\n",
      "89 50592.66796875\n",
      "90 50592.66796875\n",
      "91 50592.66796875\n",
      "92 50592.66796875\n",
      "93 50592.66796875\n",
      "94 50592.66796875\n",
      "95 50592.66796875\n",
      "96 50592.66796875\n",
      "97 50592.66796875\n",
      "98 50592.66796875\n",
      "99 50592.66796875\n",
      "100 50592.66796875\n",
      "101 50592.66796875\n",
      "102 50592.66796875\n",
      "103 50592.66796875\n",
      "104 50592.66796875\n",
      "105 50592.66796875\n",
      "106 50592.66796875\n",
      "107 50592.66796875\n",
      "108 50592.66796875\n",
      "109 50592.66796875\n",
      "110 50592.66796875\n",
      "111 50592.66796875\n",
      "112 50592.66796875\n",
      "113 50592.66796875\n",
      "114 50592.66796875\n",
      "115 50592.66796875\n",
      "116 50592.66796875\n",
      "117 50592.66796875\n",
      "118 50592.66796875\n",
      "119 50592.66796875\n",
      "120 50592.66796875\n",
      "121 50592.66796875\n",
      "122 50592.66796875\n",
      "123 50592.66796875\n",
      "124 50592.66796875\n",
      "125 50592.66796875\n",
      "126 50592.66796875\n",
      "127 50592.66796875\n",
      "128 50592.66796875\n",
      "129 50592.66796875\n",
      "130 50592.66796875\n",
      "131 50592.66796875\n",
      "132 50592.66796875\n",
      "133 50592.66796875\n",
      "134 50592.66796875\n",
      "135 50592.66796875\n",
      "136 50592.66796875\n",
      "137 50592.66796875\n",
      "138 50592.66796875\n",
      "139 50592.66796875\n",
      "140 50592.66796875\n",
      "141 50592.66796875\n",
      "142 50592.66796875\n",
      "143 50592.66796875\n",
      "144 50592.66796875\n",
      "145 50592.66796875\n",
      "146 50592.66796875\n",
      "147 50592.66796875\n",
      "148 50592.66796875\n",
      "149 50592.66796875\n",
      "150 50592.66796875\n",
      "151 50592.66796875\n",
      "152 50592.66796875\n",
      "153 50592.66796875\n",
      "154 50592.66796875\n",
      "155 50592.66796875\n",
      "156 50592.66796875\n",
      "157 50592.66796875\n",
      "158 50592.66796875\n",
      "159 50592.66796875\n",
      "160 50592.66796875\n",
      "161 50592.66796875\n",
      "162 50592.66796875\n",
      "163 50592.66796875\n",
      "164 50592.66796875\n",
      "165 50592.66796875\n",
      "166 50592.66796875\n",
      "167 50592.66796875\n",
      "168 50592.66796875\n",
      "169 50592.66796875\n",
      "170 50592.66796875\n",
      "171 50592.66796875\n",
      "172 50592.66796875\n",
      "173 50592.66796875\n",
      "174 50592.66796875\n",
      "175 50592.66796875\n",
      "176 50592.66796875\n",
      "177 50592.66796875\n",
      "178 50592.66796875\n",
      "179 50592.66796875\n",
      "180 50592.66796875\n",
      "181 50592.66796875\n",
      "182 50592.66796875\n",
      "183 50592.66796875\n",
      "184 50592.66796875\n",
      "185 50592.66796875\n",
      "186 50592.66796875\n",
      "187 50592.66796875\n",
      "188 50592.66796875\n",
      "189 50592.66796875\n",
      "190 50592.66796875\n",
      "191 50592.66796875\n",
      "192 50592.66796875\n",
      "193 50592.66796875\n",
      "194 50592.66796875\n",
      "195 50592.66796875\n",
      "196 50592.66796875\n",
      "197 50592.66796875\n",
      "198 50592.66796875\n",
      "199 50592.66796875\n",
      "200 50592.66796875\n",
      "201 50592.66796875\n",
      "202 50592.66796875\n",
      "203 50592.66796875\n",
      "204 50592.66796875\n",
      "205 50592.66796875\n",
      "206 50592.66796875\n",
      "207 50592.66796875\n",
      "208 50592.66796875\n",
      "209 50592.66796875\n",
      "210 50592.66796875\n",
      "211 50592.66796875\n",
      "212 50592.66796875\n",
      "213 50592.66796875\n",
      "214 50592.66796875\n",
      "215 50592.66796875\n",
      "216 50592.66796875\n",
      "217 50592.66796875\n",
      "218 50592.66796875\n",
      "219 50592.66796875\n",
      "220 50592.66796875\n",
      "221 50592.66796875\n",
      "222 50592.66796875\n",
      "223 50592.66796875\n",
      "224 50592.66796875\n",
      "225 50592.66796875\n",
      "226 50592.66796875\n",
      "227 50592.66796875\n",
      "228 50592.66796875\n",
      "229 50592.66796875\n",
      "230 50592.66796875\n",
      "231 50592.66796875\n",
      "232 50592.66796875\n",
      "233 50592.66796875\n",
      "234 50592.66796875\n",
      "235 50592.66796875\n",
      "236 50592.66796875\n",
      "237 50592.66796875\n",
      "238 50592.66796875\n",
      "239 50592.66796875\n",
      "240 50592.66796875\n",
      "241 50592.66796875\n",
      "242 50592.66796875\n",
      "243 50592.66796875\n",
      "244 50592.66796875\n",
      "245 50592.66796875\n",
      "246 50592.66796875\n",
      "247 50592.66796875\n",
      "248 50592.66796875\n",
      "249 50592.66796875\n",
      "250 50592.66796875\n",
      "251 50592.66796875\n",
      "252 50592.66796875\n",
      "253 50592.66796875\n",
      "254 50592.66796875\n",
      "255 50592.66796875\n",
      "256 50592.66796875\n",
      "257 50592.66796875\n",
      "258 50592.66796875\n",
      "259 50592.66796875\n",
      "260 50592.66796875\n",
      "261 50592.66796875\n",
      "262 50592.66796875\n",
      "263 50592.66796875\n",
      "264 50592.66796875\n",
      "265 50592.66796875\n",
      "266 50592.66796875\n",
      "267 50592.66796875\n",
      "268 50592.66796875\n",
      "269 50592.66796875\n",
      "270 50592.66796875\n",
      "271 50592.66796875\n",
      "272 50592.66796875\n",
      "273 50592.66796875\n",
      "274 50592.66796875\n",
      "275 50592.66796875\n",
      "276 50592.66796875\n",
      "277 50592.66796875\n",
      "278 50592.66796875\n",
      "279 50592.66796875\n",
      "280 50592.66796875\n",
      "281 50592.66796875\n",
      "282 50592.66796875\n",
      "283 50592.66796875\n",
      "284 50592.66796875\n",
      "285 50592.66796875\n",
      "286 50592.66796875\n",
      "287 50592.66796875\n",
      "288 50592.66796875\n",
      "289 50592.66796875\n",
      "290 50592.66796875\n",
      "291 50592.66796875\n",
      "292 50592.66796875\n",
      "293 50592.66796875\n",
      "294 50592.66796875\n",
      "295 50592.66796875\n",
      "296 50592.66796875\n",
      "297 50592.66796875\n",
      "298 50592.66796875\n",
      "299 50592.66796875\n",
      "300 50592.66796875\n",
      "301 50592.66796875\n",
      "302 50592.66796875\n",
      "303 50592.66796875\n",
      "304 50592.66796875\n",
      "305 50592.66796875\n",
      "306 50592.66796875\n",
      "307 50592.66796875\n",
      "308 50592.66796875\n",
      "309 50592.66796875\n",
      "310 50592.66796875\n",
      "311 50592.66796875\n",
      "312 50592.66796875\n",
      "313 50592.66796875\n",
      "314 50592.66796875\n",
      "315 50592.66796875\n",
      "316 50592.66796875\n",
      "317 50592.66796875\n",
      "318 50592.66796875\n",
      "319 50592.66796875\n",
      "320 50592.66796875\n",
      "321 50592.66796875\n",
      "322 50592.66796875\n",
      "323 50592.66796875\n",
      "324 50592.66796875\n",
      "325 50592.66796875\n",
      "326 50592.66796875\n",
      "327 50592.66796875\n",
      "328 50592.66796875\n",
      "329 50592.66796875\n",
      "330 50592.66796875\n",
      "331 50592.66796875\n",
      "332 50592.66796875\n",
      "333 50592.66796875\n",
      "334 50592.66796875\n",
      "335 50592.66796875\n",
      "336 50592.66796875\n",
      "337 50592.66796875\n",
      "338 50592.66796875\n",
      "339 50592.66796875\n",
      "340 50592.66796875\n",
      "341 50592.66796875\n",
      "342 50592.66796875\n",
      "343 50592.66796875\n",
      "344 50592.66796875\n",
      "345 50592.66796875\n",
      "346 50592.66796875\n",
      "347 50592.66796875\n",
      "348 50592.66796875\n",
      "349 50592.66796875\n",
      "350 50592.66796875\n",
      "351 50592.66796875\n",
      "352 50592.66796875\n",
      "353 50592.66796875\n",
      "354 50592.66796875\n",
      "355 50592.66796875\n",
      "356 50592.66796875\n",
      "357 50592.66796875\n",
      "358 50592.66796875\n",
      "359 50592.66796875\n",
      "360 50592.66796875\n",
      "361 50592.66796875\n",
      "362 50592.66796875\n",
      "363 50592.66796875\n",
      "364 50592.66796875\n",
      "365 50592.66796875\n",
      "366 50592.66796875\n",
      "367 50592.66796875\n",
      "368 50592.66796875\n",
      "369 50592.66796875\n",
      "370 50592.66796875\n",
      "371 50592.66796875\n",
      "372 50592.66796875\n",
      "373 50592.66796875\n",
      "374 50592.66796875\n",
      "375 50592.66796875\n",
      "376 50592.66796875\n",
      "377 50592.66796875\n",
      "378 50592.66796875\n",
      "379 50592.66796875\n",
      "380 50592.66796875\n",
      "381 50592.66796875\n",
      "382 50592.66796875\n",
      "383 50592.66796875\n",
      "384 50592.66796875\n",
      "385 50592.66796875\n",
      "386 50592.66796875\n",
      "387 50592.66796875\n",
      "388 50592.66796875\n",
      "389 50592.66796875\n",
      "390 50592.66796875\n",
      "391 50592.66796875\n",
      "392 50592.66796875\n",
      "393 50592.66796875\n",
      "394 50592.66796875\n",
      "395 50592.66796875\n",
      "396 50592.66796875\n",
      "397 50592.66796875\n",
      "398 50592.66796875\n",
      "399 50592.66796875\n",
      "400 50592.66796875\n",
      "401 50592.66796875\n",
      "402 50592.66796875\n",
      "403 50592.66796875\n",
      "404 50592.66796875\n",
      "405 50592.66796875\n",
      "406 50592.66796875\n",
      "407 50592.66796875\n",
      "408 50592.66796875\n",
      "409 50592.66796875\n",
      "410 50592.66796875\n",
      "411 50592.66796875\n",
      "412 50592.66796875\n",
      "413 50592.66796875\n",
      "414 50592.66796875\n",
      "415 50592.66796875\n",
      "416 50592.66796875\n",
      "417 50592.66796875\n",
      "418 50592.66796875\n",
      "419 50592.66796875\n",
      "420 50592.66796875\n",
      "421 50592.66796875\n",
      "422 50592.66796875\n",
      "423 50592.66796875\n",
      "424 50592.66796875\n",
      "425 50592.66796875\n",
      "426 50592.66796875\n",
      "427 50592.66796875\n",
      "428 50592.66796875\n",
      "429 50592.66796875\n",
      "430 50592.66796875\n",
      "431 50592.66796875\n",
      "432 50592.66796875\n",
      "433 50592.66796875\n",
      "434 50592.66796875\n",
      "435 50592.66796875\n",
      "436 50592.66796875\n",
      "437 50592.66796875\n",
      "438 50592.66796875\n",
      "439 50592.66796875\n",
      "440 50592.66796875\n",
      "441 50592.66796875\n",
      "442 50592.66796875\n",
      "443 50592.66796875\n",
      "444 50592.66796875\n",
      "445 50592.66796875\n",
      "446 50592.66796875\n",
      "447 50592.66796875\n",
      "448 50592.66796875\n",
      "449 50592.66796875\n",
      "450 50592.66796875\n",
      "451 50592.66796875\n",
      "452 50592.66796875\n",
      "453 50592.66796875\n",
      "454 50592.66796875\n",
      "455 50592.66796875\n",
      "456 50592.66796875\n",
      "457 50592.66796875\n",
      "458 50592.66796875\n",
      "459 50592.66796875\n",
      "460 50592.66796875\n",
      "461 50592.66796875\n",
      "462 50592.66796875\n",
      "463 50592.66796875\n",
      "464 50592.66796875\n",
      "465 50592.66796875\n",
      "466 50592.66796875\n",
      "467 50592.66796875\n",
      "468 50592.66796875\n",
      "469 50592.66796875\n",
      "470 50592.66796875\n",
      "471 50592.66796875\n",
      "472 50592.66796875\n",
      "473 50592.66796875\n",
      "474 50592.66796875\n",
      "475 50592.66796875\n",
      "476 50592.66796875\n",
      "477 50592.66796875\n",
      "478 50592.66796875\n",
      "479 50592.66796875\n",
      "480 50592.66796875\n",
      "481 50592.66796875\n",
      "482 50592.66796875\n",
      "483 50592.66796875\n",
      "484 50592.66796875\n",
      "485 50592.66796875\n",
      "486 50592.66796875\n",
      "487 50592.66796875\n",
      "488 50592.66796875\n",
      "489 50592.66796875\n",
      "490 50592.66796875\n",
      "491 50592.66796875\n",
      "492 50592.66796875\n",
      "493 50592.66796875\n",
      "494 50592.66796875\n",
      "495 50592.66796875\n",
      "496 50592.66796875\n",
      "497 50592.66796875\n",
      "498 50592.66796875\n",
      "499 50592.66796875\n"
     ]
    }
   ],
   "source": [
    "# Code: two_layer_net_amp.py\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "scaler = GradScaler()  # For automatic mixed precision\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass under autocast for mixed precision\n",
    "    with autocast():\n",
    "        h = x.mm(w1)\n",
    "        h_relu = torch.relu(h)\n",
    "        y_pred = h_relu.mm(w2)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Backward pass with scaler\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Update weights using scaler and no_grad\n",
    "    with torch.no_grad():\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b48783-caaf-459e-b599-cfc6cf090b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28674472.0\n",
      "1 22100178.0\n",
      "2 20705210.0\n",
      "3 21082048.0\n",
      "4 21247376.0\n",
      "5 19573704.0\n",
      "6 15988871.0\n",
      "7 11441961.0\n",
      "8 7432249.0\n",
      "9 4562696.5\n",
      "10 2798877.5\n",
      "11 1780412.875\n",
      "12 1205988.625\n",
      "13 873037.0\n",
      "14 670717.375\n",
      "15 539382.5625\n",
      "16 448152.0625\n",
      "17 380657.1875\n",
      "18 328104.78125\n",
      "19 285656.875\n",
      "20 250466.515625\n",
      "21 220776.46875\n",
      "22 195410.921875\n",
      "23 173550.125\n",
      "24 154597.671875\n",
      "25 138080.875\n",
      "26 123620.78125\n",
      "27 110937.0078125\n",
      "28 99758.375\n",
      "29 89883.25\n",
      "30 81132.859375\n",
      "31 73369.5078125\n",
      "32 66451.15625\n",
      "33 60271.5546875\n",
      "34 54742.96875\n",
      "35 49790.625\n",
      "36 45342.55078125\n",
      "37 41351.5703125\n",
      "38 37757.26953125\n",
      "39 34514.08203125\n",
      "40 31582.283203125\n",
      "41 28928.29296875\n",
      "42 26522.947265625\n",
      "43 24339.8515625\n",
      "44 22356.3046875\n",
      "45 20553.705078125\n",
      "46 18913.765625\n",
      "47 17418.302734375\n",
      "48 16052.8671875\n",
      "49 14808.05078125\n",
      "50 13671.322265625\n",
      "51 12629.201171875\n",
      "52 11674.2802734375\n",
      "53 10798.2861328125\n",
      "54 9994.0341796875\n",
      "55 9255.259765625\n",
      "56 8575.712890625\n",
      "57 7950.86669921875\n",
      "58 7375.19091796875\n",
      "59 6844.8642578125\n",
      "60 6355.640625\n",
      "61 5904.31982421875\n",
      "62 5487.59228515625\n",
      "63 5102.8369140625\n",
      "64 4747.2578125\n",
      "65 4418.3642578125\n",
      "66 4113.94091796875\n",
      "67 3832.03857421875\n",
      "68 3570.856689453125\n",
      "69 3328.84716796875\n",
      "70 3104.468505859375\n",
      "71 2896.25244140625\n",
      "72 2702.914306640625\n",
      "73 2523.48583984375\n",
      "74 2356.68115234375\n",
      "75 2201.623046875\n",
      "76 2057.47607421875\n",
      "77 1923.371826171875\n",
      "78 1798.558349609375\n",
      "79 1682.3553466796875\n",
      "80 1574.2088623046875\n",
      "81 1473.5145263671875\n",
      "82 1379.6922607421875\n",
      "83 1292.1673583984375\n",
      "84 1210.5413818359375\n",
      "85 1134.41845703125\n",
      "86 1063.3685302734375\n",
      "87 997.0728759765625\n",
      "88 935.148193359375\n",
      "89 877.306884765625\n",
      "90 823.2450561523438\n",
      "91 772.6820678710938\n",
      "92 725.4110107421875\n",
      "93 681.1976928710938\n",
      "94 639.8348999023438\n",
      "95 601.1044921875\n",
      "96 564.8359375\n",
      "97 530.87548828125\n",
      "98 499.0548400878906\n",
      "99 469.2704772949219\n",
      "100 441.3261413574219\n",
      "101 415.1243591308594\n",
      "102 390.5607604980469\n",
      "103 367.52978515625\n",
      "104 345.9146423339844\n",
      "105 325.6312255859375\n",
      "106 306.58837890625\n",
      "107 288.7233581542969\n",
      "108 271.9497985839844\n",
      "109 256.1841735839844\n",
      "110 241.37135314941406\n",
      "111 227.45217895507812\n",
      "112 214.3714599609375\n",
      "113 202.07455444335938\n",
      "114 190.51336669921875\n",
      "115 179.63888549804688\n",
      "116 169.41294860839844\n",
      "117 159.79608154296875\n",
      "118 150.74319458007812\n",
      "119 142.23626708984375\n",
      "120 134.22531127929688\n",
      "121 126.6827621459961\n",
      "122 119.57991790771484\n",
      "123 112.8907699584961\n",
      "124 106.58941650390625\n",
      "125 100.65658569335938\n",
      "126 95.06671905517578\n",
      "127 89.7929458618164\n",
      "128 84.82672882080078\n",
      "129 80.14263916015625\n",
      "130 75.7279052734375\n",
      "131 71.56576538085938\n",
      "132 67.63929748535156\n",
      "133 63.935726165771484\n",
      "134 60.44623947143555\n",
      "135 57.14927291870117\n",
      "136 54.0382194519043\n",
      "137 51.102020263671875\n",
      "138 48.331626892089844\n",
      "139 45.7153434753418\n",
      "140 43.24467086791992\n",
      "141 40.91285705566406\n",
      "142 38.712825775146484\n",
      "143 36.63309097290039\n",
      "144 34.667842864990234\n",
      "145 32.810665130615234\n",
      "146 31.05653190612793\n",
      "147 29.400209426879883\n",
      "148 27.838111877441406\n",
      "149 26.36252784729004\n",
      "150 24.967365264892578\n",
      "151 23.648029327392578\n",
      "152 22.400535583496094\n",
      "153 21.221160888671875\n",
      "154 20.104209899902344\n",
      "155 19.04826545715332\n",
      "156 18.049110412597656\n",
      "157 17.104280471801758\n",
      "158 16.210186004638672\n",
      "159 15.363858222961426\n",
      "160 14.562631607055664\n",
      "161 13.803926467895508\n",
      "162 13.086518287658691\n",
      "163 12.40709114074707\n",
      "164 11.763859748840332\n",
      "165 11.154318809509277\n",
      "166 10.577611923217773\n",
      "167 10.031200408935547\n",
      "168 9.51343822479248\n",
      "169 9.023476600646973\n",
      "170 8.558669090270996\n",
      "171 8.118595123291016\n",
      "172 7.702354907989502\n",
      "173 7.307605743408203\n",
      "174 6.933166027069092\n",
      "175 6.57824182510376\n",
      "176 6.242393493652344\n",
      "177 5.923250198364258\n",
      "178 5.621262073516846\n",
      "179 5.334718227386475\n",
      "180 5.063619613647461\n",
      "181 4.806404113769531\n",
      "182 4.562241554260254\n",
      "183 4.330542087554932\n",
      "184 4.111317157745361\n",
      "185 3.903197765350342\n",
      "186 3.705751657485962\n",
      "187 3.518662929534912\n",
      "188 3.3409693241119385\n",
      "189 3.1726202964782715\n",
      "190 3.01239013671875\n",
      "191 2.8608601093292236\n",
      "192 2.7170848846435547\n",
      "193 2.5804476737976074\n",
      "194 2.4509904384613037\n",
      "195 2.3282623291015625\n",
      "196 2.2114574909210205\n",
      "197 2.100574016571045\n",
      "198 1.9956156015396118\n",
      "199 1.8958992958068848\n",
      "200 1.8011213541030884\n",
      "201 1.7112787961959839\n",
      "202 1.6259773969650269\n",
      "203 1.5449377298355103\n",
      "204 1.4679745435714722\n",
      "205 1.3949131965637207\n",
      "206 1.3255342245101929\n",
      "207 1.2597697973251343\n",
      "208 1.1973440647125244\n",
      "209 1.1379754543304443\n",
      "210 1.0814354419708252\n",
      "211 1.0279552936553955\n",
      "212 0.9770907163619995\n",
      "213 0.9287651181221008\n",
      "214 0.8830283880233765\n",
      "215 0.8393344283103943\n",
      "216 0.7979530692100525\n",
      "217 0.7585378289222717\n",
      "218 0.7211551070213318\n",
      "219 0.685653567314148\n",
      "220 0.6519455909729004\n",
      "221 0.6198669075965881\n",
      "222 0.5893664360046387\n",
      "223 0.56036376953125\n",
      "224 0.5329505205154419\n",
      "225 0.506790041923523\n",
      "226 0.48196759819984436\n",
      "227 0.45832568407058716\n",
      "228 0.4358653426170349\n",
      "229 0.4145761728286743\n",
      "230 0.3942626416683197\n",
      "231 0.3749529719352722\n",
      "232 0.3566567003726959\n",
      "233 0.3392619490623474\n",
      "234 0.322709321975708\n",
      "235 0.3069442808628082\n",
      "236 0.29199352860450745\n",
      "237 0.27784082293510437\n",
      "238 0.2643100619316101\n",
      "239 0.2514241933822632\n",
      "240 0.23920279741287231\n",
      "241 0.22758640348911285\n",
      "242 0.2165221869945526\n",
      "243 0.20601382851600647\n",
      "244 0.19600291550159454\n",
      "245 0.18647229671478271\n",
      "246 0.17741325497627258\n",
      "247 0.1688520908355713\n",
      "248 0.16065654158592224\n",
      "249 0.15288546681404114\n",
      "250 0.14550530910491943\n",
      "251 0.1384478062391281\n",
      "252 0.1317707747220993\n",
      "253 0.12540897727012634\n",
      "254 0.11933281272649765\n",
      "255 0.1135658249258995\n",
      "256 0.10809125751256943\n",
      "257 0.10284996032714844\n",
      "258 0.09790951758623123\n",
      "259 0.09317628294229507\n",
      "260 0.0887051597237587\n",
      "261 0.08443769812583923\n",
      "262 0.08039028197526932\n",
      "263 0.07649365067481995\n",
      "264 0.07282483577728271\n",
      "265 0.06933662295341492\n",
      "266 0.06599006801843643\n",
      "267 0.06283646076917648\n",
      "268 0.059807583689689636\n",
      "269 0.05695681273937225\n",
      "270 0.054233428090810776\n",
      "271 0.051618918776512146\n",
      "272 0.04915035888552666\n",
      "273 0.04680090397596359\n",
      "274 0.044564202427864075\n",
      "275 0.04244915023446083\n",
      "276 0.040419068187475204\n",
      "277 0.038479384034872055\n",
      "278 0.03664810210466385\n",
      "279 0.03491143509745598\n",
      "280 0.03323325887322426\n",
      "281 0.03165486454963684\n",
      "282 0.03016253001987934\n",
      "283 0.02873889170587063\n",
      "284 0.027353085577487946\n",
      "285 0.02605844847857952\n",
      "286 0.024817299097776413\n",
      "287 0.023642029613256454\n",
      "288 0.022519169375300407\n",
      "289 0.021449435502290726\n",
      "290 0.020443418994545937\n",
      "291 0.019480984658002853\n",
      "292 0.018563106656074524\n",
      "293 0.017684660851955414\n",
      "294 0.0168642345815897\n",
      "295 0.01606217958033085\n",
      "296 0.015320743434131145\n",
      "297 0.014592449180781841\n",
      "298 0.013921081088483334\n",
      "299 0.013270546682178974\n",
      "300 0.012647186405956745\n",
      "301 0.012056690640747547\n",
      "302 0.011500786058604717\n",
      "303 0.010962317697703838\n",
      "304 0.01045480277389288\n",
      "305 0.009969605132937431\n",
      "306 0.009516077116131783\n",
      "307 0.009075469337403774\n",
      "308 0.008656274527311325\n",
      "309 0.008259681984782219\n",
      "310 0.007884289138019085\n",
      "311 0.007521843537688255\n",
      "312 0.007179418578743935\n",
      "313 0.006854622159153223\n",
      "314 0.0065486496314406395\n",
      "315 0.006251932121813297\n",
      "316 0.005969910882413387\n",
      "317 0.00570368068292737\n",
      "318 0.005439599044620991\n",
      "319 0.005199378821998835\n",
      "320 0.004970838315784931\n",
      "321 0.004747659433633089\n",
      "322 0.004540217109024525\n",
      "323 0.004344393499195576\n",
      "324 0.0041541289538145065\n",
      "325 0.003968843258917332\n",
      "326 0.0037911231629550457\n",
      "327 0.003628432285040617\n",
      "328 0.003471620613709092\n",
      "329 0.0033216974698007107\n",
      "330 0.0031789634376764297\n",
      "331 0.0030437640380114317\n",
      "332 0.002909454284235835\n",
      "333 0.00278590340167284\n",
      "334 0.002667228691279888\n",
      "335 0.0025550355203449726\n",
      "336 0.0024529569782316685\n",
      "337 0.002346856752410531\n",
      "338 0.0022525109816342592\n",
      "339 0.0021576741710305214\n",
      "340 0.002070605754852295\n",
      "341 0.001986522227525711\n",
      "342 0.001906705554574728\n",
      "343 0.001827972591854632\n",
      "344 0.0017545110313221812\n",
      "345 0.0016840874450281262\n",
      "346 0.0016159138176590204\n",
      "347 0.001550770946778357\n",
      "348 0.0014911341713741422\n",
      "349 0.0014323112554848194\n",
      "350 0.001374998944811523\n",
      "351 0.001322463620454073\n",
      "352 0.0012731986353173852\n",
      "353 0.0012236583279445767\n",
      "354 0.001176357502117753\n",
      "355 0.0011328484397381544\n",
      "356 0.0010912319412454963\n",
      "357 0.0010502311633899808\n",
      "358 0.0010117738274857402\n",
      "359 0.0009757122024893761\n",
      "360 0.0009393030777573586\n",
      "361 0.0009062235476449132\n",
      "362 0.0008732342394068837\n",
      "363 0.0008437910000793636\n",
      "364 0.0008141871076077223\n",
      "365 0.000786677876021713\n",
      "366 0.0007582299876958132\n",
      "367 0.0007316222181543708\n",
      "368 0.0007066378020681441\n",
      "369 0.0006826655007898808\n",
      "370 0.0006575558800250292\n",
      "371 0.0006363369757309556\n",
      "372 0.0006154503207653761\n",
      "373 0.0005937020760029554\n",
      "374 0.0005738224135711789\n",
      "375 0.0005549848428927362\n",
      "376 0.0005364914541132748\n",
      "377 0.0005191854434087873\n",
      "378 0.000503907329402864\n",
      "379 0.000486170087242499\n",
      "380 0.0004721125587821007\n",
      "381 0.0004566457064356655\n",
      "382 0.00044148258166387677\n",
      "383 0.00042751154978759587\n",
      "384 0.0004150196327827871\n",
      "385 0.0004017896135337651\n",
      "386 0.0003895276749972254\n",
      "387 0.0003774687647819519\n",
      "388 0.0003665252006612718\n",
      "389 0.00035582948476076126\n",
      "390 0.0003448586503509432\n",
      "391 0.0003351254854351282\n",
      "392 0.00032495814957655966\n",
      "393 0.00031593278981745243\n",
      "394 0.0003069786471314728\n",
      "395 0.00029880585498176515\n",
      "396 0.00029081449611112475\n",
      "397 0.00028204097179695964\n",
      "398 0.00027425691951066256\n",
      "399 0.00026658043498173356\n",
      "400 0.0002598492137622088\n",
      "401 0.0002527022734284401\n",
      "402 0.00024595868308097124\n",
      "403 0.000238984968746081\n",
      "404 0.00023351806157734245\n",
      "405 0.00022762507433071733\n",
      "406 0.00022072097635827959\n",
      "407 0.00021477600967045873\n",
      "408 0.00020933510677423328\n",
      "409 0.0002039840182987973\n",
      "410 0.00019855100254062563\n",
      "411 0.00019400776363909245\n",
      "412 0.00018883412121795118\n",
      "413 0.00018403725698590279\n",
      "414 0.00017952501366380602\n",
      "415 0.0001752656389726326\n",
      "416 0.00017087021842598915\n",
      "417 0.0001663979928707704\n",
      "418 0.0001622615964151919\n",
      "419 0.0001588562736287713\n",
      "420 0.00015536292630713433\n",
      "421 0.00015162024646997452\n",
      "422 0.00014814484165981412\n",
      "423 0.00014483772974926978\n",
      "424 0.0001407498202752322\n",
      "425 0.00013820076128467917\n",
      "426 0.00013549700088333338\n",
      "427 0.00013242446584627032\n",
      "428 0.00012922687164973468\n",
      "429 0.00012624780356418341\n",
      "430 0.000123706748127006\n",
      "431 0.0001210645932587795\n",
      "432 0.00011847338464576751\n",
      "433 0.00011556305980775505\n",
      "434 0.00011280343460384756\n",
      "435 0.00011112613719888031\n",
      "436 0.00010847885278053582\n",
      "437 0.000106443403637968\n",
      "438 0.0001043243901222013\n",
      "439 0.00010220133844995871\n",
      "440 9.946718637365848e-05\n",
      "441 9.75748262135312e-05\n",
      "442 9.53026392380707e-05\n",
      "443 9.369579493068159e-05\n",
      "444 9.193566074827686e-05\n",
      "445 9.03111940715462e-05\n",
      "446 8.822203380987048e-05\n",
      "447 8.630395313957706e-05\n",
      "448 8.466586587019265e-05\n",
      "449 8.296851592604071e-05\n",
      "450 8.112473005894572e-05\n",
      "451 7.99187837401405e-05\n",
      "452 7.851347618270665e-05\n",
      "453 7.691480277571827e-05\n",
      "454 7.542383536929265e-05\n",
      "455 7.424975046887994e-05\n",
      "456 7.291205110959709e-05\n",
      "457 7.168140291469172e-05\n",
      "458 7.0241789217107e-05\n",
      "459 6.883500464027748e-05\n",
      "460 6.780942203477025e-05\n",
      "461 6.673503230558708e-05\n",
      "462 6.54202449368313e-05\n",
      "463 6.441034201998264e-05\n",
      "464 6.328266317723319e-05\n",
      "465 6.208835111465305e-05\n",
      "466 6.099005258874968e-05\n",
      "467 6.0061949625378475e-05\n",
      "468 5.897036680835299e-05\n",
      "469 5.7961362472269684e-05\n",
      "470 5.712067650165409e-05\n",
      "471 5.621558739221655e-05\n",
      "472 5.5489679652964696e-05\n",
      "473 5.466894435812719e-05\n",
      "474 5.392765160650015e-05\n",
      "475 5.295112350722775e-05\n",
      "476 5.2085928473388776e-05\n",
      "477 5.123221126268618e-05\n",
      "478 5.056445297668688e-05\n",
      "479 4.9835183745017275e-05\n",
      "480 4.90891034132801e-05\n",
      "481 4.84027223137673e-05\n",
      "482 4.787277794093825e-05\n",
      "483 4.696279938798398e-05\n",
      "484 4.6541550545953214e-05\n",
      "485 4.58073009212967e-05\n",
      "486 4.5006345317233354e-05\n",
      "487 4.4400920160114765e-05\n",
      "488 4.3735752115026116e-05\n",
      "489 4.315240585128777e-05\n",
      "490 4.2520303395576775e-05\n",
      "491 4.200068724458106e-05\n",
      "492 4.154607449891046e-05\n",
      "493 4.1039485950022936e-05\n",
      "494 4.048475238960236e-05\n",
      "495 3.9788174035493284e-05\n",
      "496 3.9124908653320745e-05\n",
      "497 3.867156192427501e-05\n",
      "498 3.8200654671527445e-05\n",
      "499 3.771356205106713e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Cache the input tensor for the backward pass\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve the cached tensor from the context\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    "\n",
    "# Define input, output dimensions, and random data\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Use custom ReLU in forward pass\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb70366-7286-44b9-94ea-9760c0437917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 17:05:37.027505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-16 17:05:37.028271: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 2.5016636848449707\n",
      "Step 10, Loss: 1.5964473485946655\n",
      "Step 20, Loss: 0.9760836958885193\n",
      "Step 30, Loss: 0.5803492665290833\n",
      "Step 40, Loss: 0.3403693437576294\n",
      "Step 50, Loss: 0.19694332778453827\n",
      "Step 60, Loss: 0.11284787952899933\n",
      "Step 70, Loss: 0.06403757631778717\n",
      "Step 80, Loss: 0.036198221147060394\n",
      "Step 90, Loss: 0.02056443877518177\n",
      "Step 100, Loss: 0.011755162850022316\n",
      "Step 110, Loss: 0.006751114968210459\n",
      "Step 120, Loss: 0.003943950869143009\n",
      "Step 130, Loss: 0.002344132401049137\n",
      "Step 140, Loss: 0.0014119803672656417\n",
      "Step 150, Loss: 0.0008625935297459364\n",
      "Step 160, Loss: 0.0005376944318413734\n",
      "Step 170, Loss: 0.0003402612928766757\n",
      "Step 180, Loss: 0.0002157592389266938\n",
      "Step 190, Loss: 0.0001359271991532296\n",
      "Step 200, Loss: 8.459204400423914e-05\n",
      "Step 210, Loss: 5.1843453547917306e-05\n",
      "Step 220, Loss: 3.131864286842756e-05\n",
      "Step 230, Loss: 1.860739939729683e-05\n",
      "Step 240, Loss: 1.085907297238009e-05\n",
      "Step 250, Loss: 6.224881417438155e-06\n",
      "Step 260, Loss: 3.503816515149083e-06\n",
      "Step 270, Loss: 1.9366584638191853e-06\n",
      "Step 280, Loss: 1.050928290169395e-06\n",
      "Step 290, Loss: 5.597970584858558e-07\n",
      "Step 300, Loss: 2.92741475504954e-07\n",
      "Step 310, Loss: 1.5025253219391743e-07\n",
      "Step 320, Loss: 7.56795159873036e-08\n",
      "Step 330, Loss: 3.73906807737967e-08\n",
      "Step 340, Loss: 1.8120408284971745e-08\n",
      "Step 350, Loss: 8.607440094010599e-09\n",
      "Step 360, Loss: 4.0079100038781235e-09\n",
      "Step 370, Loss: 1.8267102275260072e-09\n",
      "Step 380, Loss: 8.176419541428004e-10\n",
      "Step 390, Loss: 3.590605035519445e-10\n",
      "Step 400, Loss: 1.5503789918547284e-10\n",
      "Step 410, Loss: 6.692404269648122e-11\n",
      "Step 420, Loss: 2.951839120357569e-11\n",
      "Step 430, Loss: 1.3478769315955486e-11\n",
      "Step 440, Loss: 6.673299933479537e-12\n",
      "Step 450, Loss: 3.7767575525315955e-12\n",
      "Step 460, Loss: 2.387466092879098e-12\n",
      "Step 470, Loss: 1.6244107838361832e-12\n",
      "Step 480, Loss: 1.2089788805486057e-12\n",
      "Step 490, Loss: 9.509452687100906e-13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Input, output dimensions and placeholders\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Input data and labels (using tf.random to simulate data)\n",
    "x = tf.random.normal((N, D_in))\n",
    "y = tf.random.normal((N, D_out))\n",
    "\n",
    "# Define the model using the Sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(H, activation='relu'),\n",
    "    tf.keras.layers.Dense(D_out)\n",
    "])\n",
    "\n",
    "# Define a loss function\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for t in range(500):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if t % 10 == 0:\n",
    "        print(f\"Step {t}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77774ce-8112-4627-b41e-cd0f6be16945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 2.4008524417877197\n",
      "Step 10, Loss: 1.5306938886642456\n",
      "Step 20, Loss: 0.9340640306472778\n",
      "Step 30, Loss: 0.5536028146743774\n",
      "Step 40, Loss: 0.32213568687438965\n",
      "Step 50, Loss: 0.18477827310562134\n",
      "Step 60, Loss: 0.10479460656642914\n",
      "Step 70, Loss: 0.059141602367162704\n",
      "Step 80, Loss: 0.03317642584443092\n",
      "Step 90, Loss: 0.01850474253296852\n",
      "Step 100, Loss: 0.010273261927068233\n",
      "Step 110, Loss: 0.005659168586134911\n",
      "Step 120, Loss: 0.0030865618027746677\n",
      "Step 130, Loss: 0.001668134587816894\n",
      "Step 140, Loss: 0.0008960291161201894\n",
      "Step 150, Loss: 0.00047824953799135983\n",
      "Step 160, Loss: 0.00025412969989702106\n",
      "Step 170, Loss: 0.00013539525389205664\n",
      "Step 180, Loss: 7.483353692805395e-05\n",
      "Step 190, Loss: 4.315172554925084e-05\n",
      "Step 200, Loss: 2.574067548266612e-05\n",
      "Step 210, Loss: 1.5804562281118706e-05\n",
      "Step 220, Loss: 9.929213774739765e-06\n",
      "Step 230, Loss: 6.341164407785982e-06\n",
      "Step 240, Loss: 4.091427854291396e-06\n",
      "Step 250, Loss: 2.6497045837459154e-06\n",
      "Step 260, Loss: 1.7140461068265722e-06\n",
      "Step 270, Loss: 1.1035787110813544e-06\n",
      "Step 280, Loss: 7.054549087115447e-07\n",
      "Step 290, Loss: 4.4691927314488566e-07\n",
      "Step 300, Loss: 2.802592575790186e-07\n",
      "Step 310, Loss: 1.7378802397161053e-07\n",
      "Step 320, Loss: 1.0648887638353699e-07\n",
      "Step 330, Loss: 6.444650324510803e-08\n",
      "Step 340, Loss: 3.850740881716774e-08\n",
      "Step 350, Loss: 2.2714653269417795e-08\n",
      "Step 360, Loss: 1.3218068417586437e-08\n",
      "Step 370, Loss: 7.59129026306482e-09\n",
      "Step 380, Loss: 4.298411404590752e-09\n",
      "Step 390, Loss: 2.403149235163937e-09\n",
      "Step 400, Loss: 1.326155296688114e-09\n",
      "Step 410, Loss: 7.221034881155219e-10\n",
      "Step 420, Loss: 3.884148558341849e-10\n",
      "Step 430, Loss: 2.0720374027671795e-10\n",
      "Step 440, Loss: 1.09643204238008e-10\n",
      "Step 450, Loss: 5.79643485987269e-11\n",
      "Step 460, Loss: 3.073817936294354e-11\n",
      "Step 470, Loss: 1.6618800086076746e-11\n",
      "Step 480, Loss: 9.29159700568949e-12\n",
      "Step 490, Loss: 5.435104189627227e-12\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define input, hidden, and output dimensions\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Random input and target data\n",
    "x = tf.random.normal((N, D_in))\n",
    "y = tf.random.normal((N, D_out))\n",
    "\n",
    "# Define a simple feedforward model using Keras Sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(H, activation='relu'),  # Hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(D_out)                  # Output layer\n",
    "])\n",
    "\n",
    "# Define loss function (mean squared error) and optimizer (Adam)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for t in range(500):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: compute predicted y\n",
    "        y_pred = model(x)\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    # Compute gradients of loss with respect to model variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # Apply gradients to update the model's weights\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if t % 10 == 0:\n",
    "        print(f\"Step {t}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71330d2a-1f3e-4aff-9bfb-8676f726270d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
